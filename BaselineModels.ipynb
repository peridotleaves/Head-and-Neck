{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BaselineModels.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO9e+Xi83YX4iFrr8FqNtsJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"2QGZd0o-iU9T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621020785645,"user_tz":240,"elapsed":20305,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"39840c5a-b7de-48c9-ffbd-9d1dcb0ed9f7"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VHAF71PLkqjp","executionInfo":{"status":"ok","timestamp":1621020787240,"user_tz":240,"elapsed":1583,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}}},"source":["import pandas as pd, numpy as np, matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from time import sleep\n","from datetime import datetime\n","\n","from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import cross_validate, GridSearchCV\n","from sklearn.ensemble import RandomForestClassifier\n","\n","from sklearn.impute import SimpleImputer\n","\n","from pathlib import Path"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"Be5jLhPeotrG","executionInfo":{"status":"ok","timestamp":1621020787242,"user_tz":240,"elapsed":1583,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}}},"source":["proj_path = 'drive/MyDrive/MIT!/Classes/6.871/6.871 Project Team Folder/'\n","eileen_path = proj_path + 'Eileen Data/'\n","joseph_path = proj_path + 'Joseph Gene Expression Data/'\n","sina_path = proj_path + 'Sina Data/'"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"id":"wtxriqJpotZr","executionInfo":{"status":"ok","timestamp":1620782195917,"user_tz":240,"elapsed":96923,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"8cf1640b-b68a-49ce-c596-a0229b9e0a98"},"source":["rna_path = Path(proj_path + 'normalized-rna-data.xlsx')\n","rna_df = pd.read_excel(rna_path)\n","rna_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Gene</th>\n","      <th>TCGA-4P-AA8J</th>\n","      <th>TCGA-BA-4074</th>\n","      <th>TCGA-BA-4075</th>\n","      <th>TCGA-BA-4076</th>\n","      <th>TCGA-BA-4077</th>\n","      <th>TCGA-BA-4078</th>\n","      <th>TCGA-BA-5149</th>\n","      <th>TCGA-BA-5151</th>\n","      <th>TCGA-BA-5152</th>\n","      <th>TCGA-BA-5153</th>\n","      <th>TCGA-BA-5555</th>\n","      <th>TCGA-BA-5556</th>\n","      <th>TCGA-BA-5557</th>\n","      <th>TCGA-BA-5558</th>\n","      <th>TCGA-BA-5559</th>\n","      <th>TCGA-BA-6868</th>\n","      <th>TCGA-BA-6869</th>\n","      <th>TCGA-BA-6870</th>\n","      <th>TCGA-BA-6871</th>\n","      <th>TCGA-BA-6872</th>\n","      <th>TCGA-BA-6873</th>\n","      <th>TCGA-BA-7269</th>\n","      <th>TCGA-BA-A4IF</th>\n","      <th>TCGA-BA-A4IG</th>\n","      <th>TCGA-BA-A4IH</th>\n","      <th>TCGA-BA-A4II</th>\n","      <th>TCGA-BA-A6D8</th>\n","      <th>TCGA-BA-A6DA</th>\n","      <th>TCGA-BA-A6DB</th>\n","      <th>TCGA-BA-A6DD</th>\n","      <th>TCGA-BA-A6DE</th>\n","      <th>TCGA-BA-A6DG</th>\n","      <th>TCGA-BA-A6DI</th>\n","      <th>TCGA-BA-A6DJ</th>\n","      <th>TCGA-BA-A6DL</th>\n","      <th>TCGA-BA-A8YP</th>\n","      <th>TCGA-BB-4217</th>\n","      <th>TCGA-BB-4223</th>\n","      <th>TCGA-BB-4224</th>\n","      <th>...</th>\n","      <th>TCGA-QK-A8Z8</th>\n","      <th>TCGA-QK-A8Z9</th>\n","      <th>TCGA-QK-A8ZA</th>\n","      <th>TCGA-QK-A8ZB</th>\n","      <th>TCGA-QK-AA3J</th>\n","      <th>TCGA-QK-AA3K</th>\n","      <th>TCGA-RS-A6TO</th>\n","      <th>TCGA-RS-A6TP</th>\n","      <th>TCGA-T2-A6WX</th>\n","      <th>TCGA-T2-A6WZ</th>\n","      <th>TCGA-T2-A6X0</th>\n","      <th>TCGA-T2-A6X2</th>\n","      <th>TCGA-T3-A92M</th>\n","      <th>TCGA-T3-A92N</th>\n","      <th>TCGA-TN-A7HI</th>\n","      <th>TCGA-TN-A7HJ</th>\n","      <th>TCGA-TN-A7HL</th>\n","      <th>TCGA-UF-A718</th>\n","      <th>TCGA-UF-A719</th>\n","      <th>TCGA-UF-A71A</th>\n","      <th>TCGA-UF-A71A.1</th>\n","      <th>TCGA-UF-A71B</th>\n","      <th>TCGA-UF-A71D</th>\n","      <th>TCGA-UF-A71E</th>\n","      <th>TCGA-UF-A7J9</th>\n","      <th>TCGA-UF-A7JA</th>\n","      <th>TCGA-UF-A7JC</th>\n","      <th>TCGA-UF-A7JD</th>\n","      <th>TCGA-UF-A7JF</th>\n","      <th>TCGA-UF-A7JH</th>\n","      <th>TCGA-UF-A7JJ</th>\n","      <th>TCGA-UF-A7JK</th>\n","      <th>TCGA-UF-A7JO</th>\n","      <th>TCGA-UF-A7JS</th>\n","      <th>TCGA-UF-A7JT</th>\n","      <th>TCGA-UF-A7JV</th>\n","      <th>TCGA-UP-A6WW</th>\n","      <th>TCGA-WA-A7GZ</th>\n","      <th>TCGA-WA-A7GZ.1</th>\n","      <th>TCGA-WA-A7H4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>?|100133144</td>\n","      <td>-2.668367</td>\n","      <td>-0.902774</td>\n","      <td>-0.875868</td>\n","      <td>-0.788130</td>\n","      <td>-0.720443</td>\n","      <td>0.305505</td>\n","      <td>-1.884686</td>\n","      <td>-2.273196</td>\n","      <td>-1.028562</td>\n","      <td>-2.359103</td>\n","      <td>-2.267961</td>\n","      <td>-1.734984</td>\n","      <td>-2.292535</td>\n","      <td>-2.034867</td>\n","      <td>-1.445529</td>\n","      <td>-1.107918</td>\n","      <td>-2.693288</td>\n","      <td>-4.924138</td>\n","      <td>-0.598996</td>\n","      <td>-2.701728</td>\n","      <td>-1.693326</td>\n","      <td>-1.028824</td>\n","      <td>-1.126058</td>\n","      <td>-0.177138</td>\n","      <td>0.555868</td>\n","      <td>-4.038260</td>\n","      <td>-0.418175</td>\n","      <td>-0.350013</td>\n","      <td>-3.113069</td>\n","      <td>-1.636349</td>\n","      <td>-1.669245</td>\n","      <td>-1.396426</td>\n","      <td>-0.250961</td>\n","      <td>-0.698789</td>\n","      <td>-1.235164</td>\n","      <td>-1.715348</td>\n","      <td>-2.172558</td>\n","      <td>0.692689</td>\n","      <td>-0.759041</td>\n","      <td>...</td>\n","      <td>-3.655830</td>\n","      <td>-2.671399</td>\n","      <td>-1.226739</td>\n","      <td>0.080303</td>\n","      <td>0.097108</td>\n","      <td>-2.735148</td>\n","      <td>-0.282547</td>\n","      <td>0.766106</td>\n","      <td>-0.510861</td>\n","      <td>0.365926</td>\n","      <td>-0.063281</td>\n","      <td>-0.349853</td>\n","      <td>-1.191962</td>\n","      <td>-1.413758</td>\n","      <td>-2.039966</td>\n","      <td>0.450061</td>\n","      <td>-2.250369</td>\n","      <td>-0.576201</td>\n","      <td>-0.892550</td>\n","      <td>0.412633</td>\n","      <td>-0.375885</td>\n","      <td>-0.002416</td>\n","      <td>0.012172</td>\n","      <td>-0.885552</td>\n","      <td>0.107522</td>\n","      <td>0.186019</td>\n","      <td>0.248591</td>\n","      <td>-0.595343</td>\n","      <td>-1.099779</td>\n","      <td>-0.509501</td>\n","      <td>-1.487508</td>\n","      <td>0.784834</td>\n","      <td>-1.195505</td>\n","      <td>-2.153869</td>\n","      <td>-2.552769</td>\n","      <td>-2.570372</td>\n","      <td>-0.957308</td>\n","      <td>1.103520</td>\n","      <td>1.055458</td>\n","      <td>0.110682</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>?|100134869</td>\n","      <td>-0.794596</td>\n","      <td>-1.020686</td>\n","      <td>-2.464098</td>\n","      <td>-2.221278</td>\n","      <td>-0.817997</td>\n","      <td>-0.380211</td>\n","      <td>-1.300005</td>\n","      <td>-2.467344</td>\n","      <td>-2.103687</td>\n","      <td>-1.200386</td>\n","      <td>-2.639825</td>\n","      <td>-1.386775</td>\n","      <td>-1.542003</td>\n","      <td>-0.099920</td>\n","      <td>-1.203517</td>\n","      <td>-0.184573</td>\n","      <td>-2.223283</td>\n","      <td>-1.524609</td>\n","      <td>0.511413</td>\n","      <td>-1.497810</td>\n","      <td>-0.519020</td>\n","      <td>-2.926866</td>\n","      <td>0.069059</td>\n","      <td>-0.617449</td>\n","      <td>0.294437</td>\n","      <td>-1.599741</td>\n","      <td>-1.268774</td>\n","      <td>-2.703403</td>\n","      <td>-1.258081</td>\n","      <td>-1.987165</td>\n","      <td>-2.019275</td>\n","      <td>-4.073444</td>\n","      <td>-0.064970</td>\n","      <td>-1.485542</td>\n","      <td>-1.667472</td>\n","      <td>-0.351495</td>\n","      <td>-1.863715</td>\n","      <td>0.325171</td>\n","      <td>-1.096923</td>\n","      <td>...</td>\n","      <td>-2.005872</td>\n","      <td>-1.121858</td>\n","      <td>-0.589878</td>\n","      <td>0.244041</td>\n","      <td>0.619169</td>\n","      <td>-1.347078</td>\n","      <td>-1.228849</td>\n","      <td>1.150059</td>\n","      <td>-0.165324</td>\n","      <td>-1.335510</td>\n","      <td>1.230945</td>\n","      <td>0.076515</td>\n","      <td>-0.698765</td>\n","      <td>-0.998200</td>\n","      <td>-1.801836</td>\n","      <td>0.926873</td>\n","      <td>-1.446833</td>\n","      <td>-0.321043</td>\n","      <td>-0.846905</td>\n","      <td>1.036154</td>\n","      <td>-0.526208</td>\n","      <td>-0.385914</td>\n","      <td>-1.147425</td>\n","      <td>-0.536055</td>\n","      <td>0.258005</td>\n","      <td>0.332227</td>\n","      <td>0.173918</td>\n","      <td>-1.749557</td>\n","      <td>-0.305143</td>\n","      <td>-1.010059</td>\n","      <td>-0.272792</td>\n","      <td>0.844650</td>\n","      <td>-1.583013</td>\n","      <td>-2.747920</td>\n","      <td>-3.398868</td>\n","      <td>-0.710990</td>\n","      <td>-0.290401</td>\n","      <td>1.118641</td>\n","      <td>0.730860</td>\n","      <td>0.065397</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>?|10357</td>\n","      <td>2.411792</td>\n","      <td>3.752946</td>\n","      <td>3.888404</td>\n","      <td>3.634904</td>\n","      <td>3.915315</td>\n","      <td>3.313590</td>\n","      <td>3.563117</td>\n","      <td>2.727658</td>\n","      <td>2.735800</td>\n","      <td>3.036732</td>\n","      <td>2.753468</td>\n","      <td>2.934567</td>\n","      <td>2.452501</td>\n","      <td>3.283812</td>\n","      <td>3.655104</td>\n","      <td>3.285483</td>\n","      <td>1.962820</td>\n","      <td>2.940204</td>\n","      <td>3.198316</td>\n","      <td>2.351329</td>\n","      <td>3.338756</td>\n","      <td>1.387340</td>\n","      <td>4.097065</td>\n","      <td>3.339682</td>\n","      <td>3.665524</td>\n","      <td>2.800231</td>\n","      <td>1.799342</td>\n","      <td>0.556048</td>\n","      <td>2.536214</td>\n","      <td>2.694082</td>\n","      <td>2.321353</td>\n","      <td>2.114732</td>\n","      <td>2.570874</td>\n","      <td>2.725813</td>\n","      <td>2.521753</td>\n","      <td>3.613625</td>\n","      <td>4.021507</td>\n","      <td>3.273238</td>\n","      <td>3.727679</td>\n","      <td>...</td>\n","      <td>2.515806</td>\n","      <td>1.929937</td>\n","      <td>3.203928</td>\n","      <td>1.798339</td>\n","      <td>3.225687</td>\n","      <td>2.533997</td>\n","      <td>0.494128</td>\n","      <td>3.026564</td>\n","      <td>2.555471</td>\n","      <td>1.452136</td>\n","      <td>2.279025</td>\n","      <td>2.184747</td>\n","      <td>3.471884</td>\n","      <td>3.265413</td>\n","      <td>2.644048</td>\n","      <td>2.297885</td>\n","      <td>2.424818</td>\n","      <td>1.051801</td>\n","      <td>2.293206</td>\n","      <td>4.051905</td>\n","      <td>3.576949</td>\n","      <td>2.762821</td>\n","      <td>1.628951</td>\n","      <td>2.265648</td>\n","      <td>2.014319</td>\n","      <td>2.358678</td>\n","      <td>1.522075</td>\n","      <td>2.625551</td>\n","      <td>2.010490</td>\n","      <td>2.429561</td>\n","      <td>1.744007</td>\n","      <td>2.406473</td>\n","      <td>3.157449</td>\n","      <td>-0.420827</td>\n","      <td>2.519974</td>\n","      <td>2.545822</td>\n","      <td>2.312506</td>\n","      <td>1.767151</td>\n","      <td>2.989047</td>\n","      <td>2.724889</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>?|10431</td>\n","      <td>5.945483</td>\n","      <td>6.270801</td>\n","      <td>5.988357</td>\n","      <td>5.749688</td>\n","      <td>5.322637</td>\n","      <td>5.071589</td>\n","      <td>5.728863</td>\n","      <td>6.196538</td>\n","      <td>5.694105</td>\n","      <td>5.245816</td>\n","      <td>4.967065</td>\n","      <td>5.191896</td>\n","      <td>4.908296</td>\n","      <td>5.308824</td>\n","      <td>5.207073</td>\n","      <td>5.195706</td>\n","      <td>5.975070</td>\n","      <td>5.399957</td>\n","      <td>5.708008</td>\n","      <td>5.480605</td>\n","      <td>5.582467</td>\n","      <td>6.821470</td>\n","      <td>6.130176</td>\n","      <td>5.893784</td>\n","      <td>5.690444</td>\n","      <td>5.834212</td>\n","      <td>6.357782</td>\n","      <td>5.306547</td>\n","      <td>5.648564</td>\n","      <td>6.023422</td>\n","      <td>6.712605</td>\n","      <td>5.167052</td>\n","      <td>5.104636</td>\n","      <td>5.603932</td>\n","      <td>6.236621</td>\n","      <td>4.851152</td>\n","      <td>5.798102</td>\n","      <td>5.261801</td>\n","      <td>6.403219</td>\n","      <td>...</td>\n","      <td>5.758368</td>\n","      <td>5.037845</td>\n","      <td>5.685049</td>\n","      <td>5.487995</td>\n","      <td>5.533157</td>\n","      <td>5.522161</td>\n","      <td>5.838139</td>\n","      <td>5.418221</td>\n","      <td>5.614866</td>\n","      <td>6.652706</td>\n","      <td>5.481120</td>\n","      <td>5.563760</td>\n","      <td>5.564250</td>\n","      <td>5.871047</td>\n","      <td>6.304907</td>\n","      <td>5.586194</td>\n","      <td>5.982780</td>\n","      <td>4.730400</td>\n","      <td>5.587638</td>\n","      <td>5.466640</td>\n","      <td>5.384323</td>\n","      <td>5.269697</td>\n","      <td>6.112797</td>\n","      <td>5.679450</td>\n","      <td>5.477513</td>\n","      <td>5.304144</td>\n","      <td>6.047472</td>\n","      <td>5.436376</td>\n","      <td>5.963395</td>\n","      <td>5.358525</td>\n","      <td>4.973021</td>\n","      <td>5.648660</td>\n","      <td>5.790883</td>\n","      <td>5.388906</td>\n","      <td>6.208256</td>\n","      <td>5.519225</td>\n","      <td>5.984255</td>\n","      <td>5.102151</td>\n","      <td>4.833012</td>\n","      <td>5.228947</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>?|155060</td>\n","      <td>3.523286</td>\n","      <td>0.672797</td>\n","      <td>1.733573</td>\n","      <td>2.330802</td>\n","      <td>1.187771</td>\n","      <td>3.408542</td>\n","      <td>1.881185</td>\n","      <td>1.889117</td>\n","      <td>-0.802738</td>\n","      <td>2.312371</td>\n","      <td>2.269571</td>\n","      <td>1.538062</td>\n","      <td>-0.186946</td>\n","      <td>2.914725</td>\n","      <td>2.770239</td>\n","      <td>1.205832</td>\n","      <td>2.113614</td>\n","      <td>1.798942</td>\n","      <td>2.500964</td>\n","      <td>1.176436</td>\n","      <td>2.076542</td>\n","      <td>0.313958</td>\n","      <td>2.683034</td>\n","      <td>1.444088</td>\n","      <td>1.862935</td>\n","      <td>2.745887</td>\n","      <td>2.887599</td>\n","      <td>1.424271</td>\n","      <td>1.981907</td>\n","      <td>0.655867</td>\n","      <td>2.114058</td>\n","      <td>2.856064</td>\n","      <td>3.374485</td>\n","      <td>3.844550</td>\n","      <td>2.470323</td>\n","      <td>2.376110</td>\n","      <td>2.829765</td>\n","      <td>3.902923</td>\n","      <td>4.185408</td>\n","      <td>...</td>\n","      <td>3.018713</td>\n","      <td>2.576960</td>\n","      <td>2.783646</td>\n","      <td>1.373680</td>\n","      <td>2.967856</td>\n","      <td>3.295536</td>\n","      <td>4.368545</td>\n","      <td>2.941191</td>\n","      <td>3.719969</td>\n","      <td>2.552855</td>\n","      <td>3.277967</td>\n","      <td>4.265101</td>\n","      <td>3.131309</td>\n","      <td>4.203923</td>\n","      <td>2.422933</td>\n","      <td>3.103904</td>\n","      <td>2.031128</td>\n","      <td>2.620919</td>\n","      <td>2.163159</td>\n","      <td>3.032424</td>\n","      <td>1.620391</td>\n","      <td>3.029272</td>\n","      <td>2.715181</td>\n","      <td>1.928043</td>\n","      <td>2.815603</td>\n","      <td>4.057478</td>\n","      <td>1.809895</td>\n","      <td>3.745528</td>\n","      <td>3.844495</td>\n","      <td>3.879251</td>\n","      <td>2.299783</td>\n","      <td>1.950906</td>\n","      <td>3.446527</td>\n","      <td>1.640209</td>\n","      <td>3.734540</td>\n","      <td>4.292657</td>\n","      <td>2.999944</td>\n","      <td>3.518798</td>\n","      <td>3.219246</td>\n","      <td>3.645937</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 567 columns</p>\n","</div>"],"text/plain":["          Gene  TCGA-4P-AA8J  ...  TCGA-WA-A7GZ.1  TCGA-WA-A7H4\n","0  ?|100133144     -2.668367  ...        1.055458      0.110682\n","1  ?|100134869     -0.794596  ...        0.730860      0.065397\n","2      ?|10357      2.411792  ...        2.989047      2.724889\n","3      ?|10431      5.945483  ...        4.833012      5.228947\n","4     ?|155060      3.523286  ...        3.219246      3.645937\n","\n","[5 rows x 567 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"ABV4tPSIpF-a","colab":{"base_uri":"https://localhost:8080/","height":326},"executionInfo":{"status":"ok","timestamp":1620245448774,"user_tz":240,"elapsed":518,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"460450ed-7118-4e24-8c3d-0a334e853e8b"},"source":["mut_clean_path = Path(eileen_path + 'rna&mut_data_top_400.csv')\n","mut_df = pd.read_csv(mut_clean_path)\n","mut_df = mut_df[mut_df.Consequence.apply(lambda x: 'synonymous_variant' not in x)]\n","mut_df = mut_df[mut_df['rank'] > 60]\n","mut_df.tail()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>PATIENT_ID</th>\n","      <th>binary_vital_status</th>\n","      <th>survival_days</th>\n","      <th>age_at_diagnosis</th>\n","      <th>Matched_Norm_Sample_Barcode</th>\n","      <th>gene</th>\n","      <th>rank</th>\n","      <th>Entrez_Gene_Id</th>\n","      <th>Matched_Norm_Sample_Barcode.1</th>\n","      <th>Tumor_Sample_Barcode</th>\n","      <th>Consequence</th>\n","      <th>rna_seq_count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>4048</th>\n","      <td>4048</td>\n","      <td>TCGA-IQ-7632</td>\n","      <td>0</td>\n","      <td>441.945</td>\n","      <td>68</td>\n","      <td>TCGA-IQ-7632-10</td>\n","      <td>TRIM27</td>\n","      <td>300</td>\n","      <td>5987.0</td>\n","      <td>TCGA-IQ-7632-10</td>\n","      <td>TCGA-IQ-7632-01</td>\n","      <td>missense_variant</td>\n","      <td>4.809671</td>\n","    </tr>\n","    <tr>\n","      <th>4049</th>\n","      <td>4049</td>\n","      <td>TCGA-IQ-7632</td>\n","      <td>0</td>\n","      <td>441.945</td>\n","      <td>68</td>\n","      <td>TCGA-IQ-7632-10</td>\n","      <td>PAX5</td>\n","      <td>314</td>\n","      <td>5079.0</td>\n","      <td>TCGA-IQ-7632-10</td>\n","      <td>TCGA-IQ-7632-01</td>\n","      <td>missense_variant</td>\n","      <td>0.444092</td>\n","    </tr>\n","    <tr>\n","      <th>4050</th>\n","      <td>4050</td>\n","      <td>TCGA-IQ-7632</td>\n","      <td>0</td>\n","      <td>441.945</td>\n","      <td>68</td>\n","      <td>TCGA-IQ-7632-10</td>\n","      <td>PCDH11X</td>\n","      <td>317</td>\n","      <td>27328.0</td>\n","      <td>TCGA-IQ-7632-10</td>\n","      <td>TCGA-IQ-7632-01</td>\n","      <td>missense_variant</td>\n","      <td>-4.595420</td>\n","    </tr>\n","    <tr>\n","      <th>4051</th>\n","      <td>4051</td>\n","      <td>TCGA-IQ-7632</td>\n","      <td>0</td>\n","      <td>441.945</td>\n","      <td>68</td>\n","      <td>TCGA-IQ-7632-10</td>\n","      <td>BRWD3</td>\n","      <td>355</td>\n","      <td>254065.0</td>\n","      <td>TCGA-IQ-7632-10</td>\n","      <td>TCGA-IQ-7632-01</td>\n","      <td>missense_variant</td>\n","      <td>2.332453</td>\n","    </tr>\n","    <tr>\n","      <th>4053</th>\n","      <td>4053</td>\n","      <td>TCGA-CV-6433</td>\n","      <td>0</td>\n","      <td>642.330</td>\n","      <td>57</td>\n","      <td>TCGA-CV-6433-11</td>\n","      <td>TAF1</td>\n","      <td>361</td>\n","      <td>6872.0</td>\n","      <td>TCGA-CV-6433-11</td>\n","      <td>TCGA-CV-6433-01</td>\n","      <td>missense_variant</td>\n","      <td>4.616759</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Unnamed: 0    PATIENT_ID  ...       Consequence  rna_seq_count\n","4048        4048  TCGA-IQ-7632  ...  missense_variant       4.809671\n","4049        4049  TCGA-IQ-7632  ...  missense_variant       0.444092\n","4050        4050  TCGA-IQ-7632  ...  missense_variant      -4.595420\n","4051        4051  TCGA-IQ-7632  ...  missense_variant       2.332453\n","4053        4053  TCGA-CV-6433  ...  missense_variant       4.616759\n","\n","[5 rows x 13 columns]"]},"metadata":{"tags":[]},"execution_count":74}]},{"cell_type":"markdown","metadata":{"id":"ggbpDpd-1lf7"},"source":["# Mutational Data"]},{"cell_type":"markdown","metadata":{"id":"UJStSHdI3dU0"},"source":["## Eileen Data Processing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":338},"id":"GxBn61Ek1n5e","executionInfo":{"status":"ok","timestamp":1620245488142,"user_tz":240,"elapsed":2138,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"bc4bf7b1-b3ff-4897-c688-dc5dc842259e"},"source":["one_hot_genes = pd.get_dummies(mut_df['gene'])\n","one_hot_data = mut_df[['PATIENT_ID','binary_vital_status', 'survival_days']].join(one_hot_genes)\n","patient_ids = one_hot_data.PATIENT_ID.unique()\n","metadata = one_hot_data[['PATIENT_ID','binary_vital_status', 'survival_days']].drop_duplicates().reset_index().drop(columns='index')\n","\n","dfs_to_append = [] \n","\n","for id in patient_ids: \n","  rel_col = one_hot_data[one_hot_data.PATIENT_ID == id]\n","  mut = rel_col.drop(columns=['PATIENT_ID','binary_vital_status', 'survival_days'])\n","  summed_df = mut.sum().apply(lambda x: 0 if x == 0 else 1)\n","  dfs_to_append.append(summed_df)\n","\n","combined_df = pd.concat(dfs_to_append, axis=1).T\n","final_mut_df = metadata.join(combined_df)\n","final_mut_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PATIENT_ID</th>\n","      <th>binary_vital_status</th>\n","      <th>survival_days</th>\n","      <th>ABCB9</th>\n","      <th>ACSS3</th>\n","      <th>ADAMTS3</th>\n","      <th>ADCY10</th>\n","      <th>ADRBK2</th>\n","      <th>AHNAK</th>\n","      <th>AHSG</th>\n","      <th>AK5</th>\n","      <th>ALDH1A2</th>\n","      <th>AMY2B</th>\n","      <th>ANKH</th>\n","      <th>APAF1</th>\n","      <th>ASPA</th>\n","      <th>ASXL3</th>\n","      <th>ATP1B3</th>\n","      <th>BBX</th>\n","      <th>BICC1</th>\n","      <th>BMP2K</th>\n","      <th>BRIP1</th>\n","      <th>BRWD3</th>\n","      <th>BTNL8</th>\n","      <th>C19orf18</th>\n","      <th>C1QTNF3</th>\n","      <th>C2orf88</th>\n","      <th>C4orf33</th>\n","      <th>C7orf60</th>\n","      <th>C8orf34</th>\n","      <th>C8orf76</th>\n","      <th>CADPS2</th>\n","      <th>CAPS2</th>\n","      <th>CCDC7</th>\n","      <th>CCL1</th>\n","      <th>CCND1</th>\n","      <th>CD40</th>\n","      <th>CDH8</th>\n","      <th>CDK8</th>\n","      <th>CECR2</th>\n","      <th>...</th>\n","      <th>THAP2</th>\n","      <th>THAP5</th>\n","      <th>THSD7A</th>\n","      <th>TIGD4</th>\n","      <th>TMEM167A</th>\n","      <th>TMEM2</th>\n","      <th>TMEM200A</th>\n","      <th>TMTC3</th>\n","      <th>TNFRSF11A</th>\n","      <th>TRAT1</th>\n","      <th>TRIM27</th>\n","      <th>TRIM58</th>\n","      <th>TSGA10</th>\n","      <th>TWSG1</th>\n","      <th>UFSP1</th>\n","      <th>UPF2</th>\n","      <th>UVRAG</th>\n","      <th>VEZF1</th>\n","      <th>WARS2</th>\n","      <th>YIPF7</th>\n","      <th>YOD1</th>\n","      <th>ZBTB22</th>\n","      <th>ZFP2</th>\n","      <th>ZFP36L2</th>\n","      <th>ZFP42</th>\n","      <th>ZFP91</th>\n","      <th>ZNF10</th>\n","      <th>ZNF211</th>\n","      <th>ZNF254</th>\n","      <th>ZNF285</th>\n","      <th>ZNF383</th>\n","      <th>ZNF491</th>\n","      <th>ZNF568</th>\n","      <th>ZNF623</th>\n","      <th>ZNF684</th>\n","      <th>ZNF701</th>\n","      <th>ZNF763</th>\n","      <th>ZNF846</th>\n","      <th>ZNF98</th>\n","      <th>ZRANB2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>TCGA-CV-5971</td>\n","      <td>0</td>\n","      <td>702.415</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>TCGA-CV-A468</td>\n","      <td>1</td>\n","      <td>464.820</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>TCGA-F7-A624</td>\n","      <td>0</td>\n","      <td>378.810</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>TCGA-BA-A6D8</td>\n","      <td>0</td>\n","      <td>851.560</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>TCGA-CV-7424</td>\n","      <td>1</td>\n","      <td>453.840</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 276 columns</p>\n","</div>"],"text/plain":["     PATIENT_ID  binary_vital_status  survival_days  ...  ZNF846  ZNF98  ZRANB2\n","0  TCGA-CV-5971                    0        702.415  ...       0      0       0\n","1  TCGA-CV-A468                    1        464.820  ...       0      0       0\n","2  TCGA-F7-A624                    0        378.810  ...       0      0       0\n","3  TCGA-BA-A6D8                    0        851.560  ...       0      0       0\n","4  TCGA-CV-7424                    1        453.840  ...       0      0       0\n","\n","[5 rows x 276 columns]"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"markdown","metadata":{"id":"anhCVnTN3hv2"},"source":["## Models"]},{"cell_type":"markdown","metadata":{"id":"jCalF4K73p3s"},"source":["**Binary Classification** "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IQ_SQWhp3krJ","executionInfo":{"status":"ok","timestamp":1620243635279,"user_tz":240,"elapsed":1764,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"008284f3-cfe6-49e9-e772-312619bddaa9"},"source":["X, y = final_mut_df.drop(columns=['PATIENT_ID','binary_vital_status', 'survival_days']), final_mut_df['binary_vital_status']\n","\n","clf = RandomForestClassifier()\n","cv_results = cross_validate(clf, X, y, cv=5, scoring='roc_auc')\n","print(\"RFC\", np.mean(cv_results['test_score']))\n","\n","#try l1 regularization \n","clf = LogisticRegression(penalty='l1', solver='liblinear', C=1, class_weight='balanced')\n","cv_results = cross_validate(clf, X, y, cv=5, scoring='roc_auc')\n","print(\"LR\", np.mean(cv_results['test_score']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["RFC 0.5029920212765957\n","LR 0.48470875052148515\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RTKbGIe8-N0k"},"source":["Linear Regression"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GSkQeV3I47Q9","executionInfo":{"status":"ok","timestamp":1620243665431,"user_tz":240,"elapsed":535,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"de5380ca-0389-4e56-aa78-64b301814557"},"source":["X_lreg, y_lreg = final_mut_df.drop(columns=['PATIENT_ID', 'binary_vital_status', 'survival_days']), final_mut_df['survival_days']\n","lreg = LinearRegression()\n","cv_results = cross_validate(lreg, X_lreg, y_lreg, cv=5, return_train_score = True)\n","print(\"LinReg\", np.mean(cv_results['test_score']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["LinReg -2.2683284066581046e+26\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SvEBH91NrMct"},"source":["# RNA Data"]},{"cell_type":"markdown","metadata":{"id":"0MmWcbH9-TeQ"},"source":["## Load"]},{"cell_type":"markdown","metadata":{"id":"e9CFtqe-U0KW"},"source":["### Hide Old"]},{"cell_type":"code","metadata":{"id":"gggwXXpirPVT"},"source":["# # Open normalized data\n","# df = rna_df\n","\n","# genes = df[['Gene']]\n","# features = df[df.columns[1:]].T\n","\n","# # Get Labels\n","# judith_path = Path(proj_path + \"/ITH-H_N_Judith.xlsx\")\n","# judith_df = pd.read_excel(judith_path)\n","\n","# # All Patients\n","# patients_df = judith_df[['PATIENT_ID', 'survival_days', 'binary_vital_status']]\n","\n","# #Filter out patients NOT IN RNA NORMED DATA ONLY\n","# for pat in ['TCGA-MT-A67G', 'TCGA-CQ-7064', 'TCGA-IQ-A61L', 'TCGA-CN-A63Y', 'TCGA-BA-A6DF', 'TCGA-IQ-A61K', 'TCGA-CN-A640']:\n","#   patients_df = patients_df.loc[patients_df['PATIENT_ID'] != pat, ('PATIENT_ID', 'survival_days', 'binary_vital_status')]\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V57JjTgTU2IE"},"source":["### New"]},{"cell_type":"code","metadata":{"id":"uQ6GffbWKQBT"},"source":["df = rna_df\n","\n","clean_path = Path(eileen_path + \"cleaned_clinical_data.csv\")\n","clean_df = pd.read_csv(clean_path)\n","\n","patients_df = clean_df[['PATIENT_ID', 'OS_MONTHS', 'BINARY_SURVIVAL']]\n","patients_df = patients_df[~patients_df.PATIENT_ID.isin( ['TCGA-MT-A67G', 'TCGA-CQ-7064', 'TCGA-IQ-A61L', 'TCGA-CN-A63Y', 'TCGA-BA-A6DF', 'TCGA-IQ-A61K', 'TCGA-CN-A640'])]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2qOW_ynaEL9Q"},"source":["rna_var = Path(joseph_path + \"variation.csv\")\n","rna_var_genes = pd.read_csv(rna_var)\n","rna_var_genes = rna_var_genes.iloc[:25,1]\n","rna_exp_df = df[df.Gene.isin(rna_var_genes)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QiiAzmOkNdR_","executionInfo":{"status":"ok","timestamp":1620782539755,"user_tz":240,"elapsed":2544,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"f5a5dbc9-ea71-4774-a289-48b2240f638e"},"source":["rna_var_genes\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0            MYH2|4620\n","1            MYH1|4619\n","2            MYL1|4632\n","3      C20orf114|92747\n","4          MAGEA6|4105\n","            ...       \n","995           HLF|3131\n","996        HOXA11|3207\n","997     C14orf73|91828\n","998         MPPED2|744\n","999      PCDHGB1|56104\n","Name: Gene, Length: 1000, dtype: object"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"VgxgJMefDVYi"},"source":["# Get rna norm data as features, survival days as labels\n","patients_rna = X = rna_exp_df[patients_df.PATIENT_ID].T #Uses top k genes\n","patients_vital = y = patients_df['BINARY_SURVIVAL']\n","\n","pats_df = patients_df[~patients_df.PATIENT_ID.isin(['TCGA-CQ-A4CA','TCGA-H7-A6C4'])]\n","X_lreg = rna_exp_df[pats_df.PATIENT_ID].T\n","patients_survival = y_lreg = pats_df['OS_MONTHS']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SP4eXjvPJepp"},"source":["X_p = X_lreg\n","y_p = pd.to_numeric(pats_df['OS_MONTHS']).apply(lambda x: 1 if x >= 36 else 0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y8KsWwBD_F7D"},"source":["## Models"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mzIR_oqpJ5i5","executionInfo":{"status":"ok","timestamp":1620786923852,"user_tz":240,"elapsed":1633,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"83c5cecb-7717-46e6-efb1-bbed79381218"},"source":["clf = RandomForestClassifier()\n","cv_results = cross_validate(clf, X_p, y_p, cv=5, scoring='roc_auc')\n","print(\"RFC\", np.mean(cv_results['test_score']))\n","\n","#try l1 regularization \n","clf = LogisticRegression(penalty='l1', solver='liblinear', C=0.1, class_weight='balanced')\n","cv_results = cross_validate(clf, X_p, y_p, cv=5, scoring='roc_auc')\n","print(\"LR\", np.mean(cv_results['test_score']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["RFC 0.44936124794745486\n","LR 0.4617766830870279\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6i8Yb6dkL42l","executionInfo":{"status":"ok","timestamp":1620787346233,"user_tz":240,"elapsed":7100,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"bcf5c478-ca57-4e54-b76d-44885c8cc521"},"source":["model_params = {\n","    \"penalty\":[\"l1\",\"l2\", \"elasticnet\", \"none\"],\n","    \"C\":[0.1, 0.25, 0.5, 1, 5, 10],\n","    \"solver\":[\"liblinear\", \"lbfgs\", \"saga\", \"sag\"]\n","}\n","\n","search_model = GridSearchCV(LogisticRegression(), model_params)\n","search_model.fit(X_p, y_p)\n","search_model.best_params_"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: penalty='none' is not supported for the liblinear solver\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: penalty='none' is not supported for the liblinear solver\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: penalty='none' is not supported for the liblinear solver\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: penalty='none' is not supported for the liblinear solver\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: penalty='none' is not supported for the liblinear solver\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: penalty='none' is not supported for the liblinear solver\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio \"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["{'C': 0.1, 'penalty': 'l1', 'solver': 'saga'}"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"markdown","metadata":{"id":"bnVE6NfuNSdF"},"source":["**Binary Classification**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b-idGbHD_KJN","executionInfo":{"status":"ok","timestamp":1620788904971,"user_tz":240,"elapsed":1862,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"bbe97602-ecf6-4adf-d78f-b72c694170b7"},"source":["clf = RandomForestClassifier()\n","cv_results = cross_validate(clf, X, y, cv=5, scoring='roc_auc')\n","print(\"RFC\", np.mean(cv_results['test_score']))\n","\n","#try l1 regularization \n","clf = LogisticRegression(penalty='l1', solver='saga', C=0.25, class_weight='balanced')\n","cv_results = cross_validate(clf, X, y, cv=5, scoring='roc_auc')\n","print(\"LR\", np.mean(cv_results['test_score']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["RFC 0.5487929292929292\n","LR 0.6067239057239058\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"_w1lQiQrMgKK"},"source":["**Results** \\\\\n","**RFC | LR**\n","\n","400: \\\\\n","60 | 56\n","\n","200: \\\\\n","62 | 59\n","\n","100: \\\\\n","59 | 61 or\n","61 | 61\n","\n","50: \\\\\n","60 | 61\n","\n","25: \\\\\n","56 | 62\n","\n","10: \\\\\n","52 | 57"]},{"cell_type":"markdown","metadata":{"id":"_y8nsYULILY7"},"source":["**OLD** \\\\\n","All Genes: \\\\\n","RFC 62 \\\\\n","LR 60\n","\n","400 Genes: \\\\\n","RFC 61 \\\\\n","LR 54\n","\n","200 Genes:\n","RFC 63 \\\\\n","LR 59\n","\n","100 Genes: \\\\\n","RFC 61 \\\\\n","LR 63\n","\n","50 Genes: \\\\\n","RFC 59 \\\\\n","LR 61"]},{"cell_type":"markdown","metadata":{"id":"O5Llx13fN2PT"},"source":["**Linear Regression**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2p3uXmMcCSkZ","executionInfo":{"status":"ok","timestamp":1620247194495,"user_tz":240,"elapsed":463,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"526e637c-363b-4d2a-b307-d2bb69710dc4"},"source":["lreg = LinearRegression()\n","cv_results = cross_validate(lreg, X_lreg, y_lreg, cv=5, return_train_score = True)\n","print(\"LinReg\", np.mean(cv_results['test_score']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["LinReg -0.22412631642163516\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VUXTglGcPxra"},"source":["**NEW**\n","\n","400: -56.6\n","\n","200: -1.97\n","\n","100: -0.71\n","\n","50: -0.37\n","\n","25: -0.22\n","\n","10: -0.22"]},{"cell_type":"markdown","metadata":{"id":"1Xn97O8CIUT7"},"source":["**OLD**\n","\n","All genes:\n","-0.31\n","\n","400 Genes:\n","-14.17\n","\n","200 Genes:\n","-2.76\n","\n","100 Genes:\n","-0.925\n","\n","50 Genes:\n","-0.376"]},{"cell_type":"markdown","metadata":{"id":"lLq9enD1PYZY"},"source":["# Methylation Data"]},{"cell_type":"markdown","metadata":{"id":"sXe9iQ0EPazf"},"source":["## Load Data"]},{"cell_type":"code","metadata":{"id":"iBhCspETK5ol","colab":{"base_uri":"https://localhost:8080/","height":287},"executionInfo":{"status":"ok","timestamp":1620498367377,"user_tz":240,"elapsed":2176,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"4d800155-859a-46f4-c4e6-77ec9f1f4e00"},"source":["#df = rna_df\n","\n","clean_path = Path(eileen_path + \"cleaned_clinical_data.csv\")\n","clean_df = pd.read_csv(clean_path)\n","patients_df = clean_df[['PATIENT_ID', 'OS_MONTHS', 'BINARY_SURVIVAL']]\n","\n","meth_path = Path(sina_path + \"top1000var_meth.csv\")\n","meth_df = pd.read_csv(meth_path)\n","meth_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Hugo_Symbol</th>\n","      <th>Entrez_Gene_Id</th>\n","      <th>variation</th>\n","      <th>TCGA-4P-AA8J-01</th>\n","      <th>TCGA-BA-4074-01</th>\n","      <th>TCGA-BA-4075-01</th>\n","      <th>TCGA-BA-4076-01</th>\n","      <th>TCGA-BA-4077-01</th>\n","      <th>TCGA-BA-4078-01</th>\n","      <th>TCGA-BA-5149-01</th>\n","      <th>TCGA-BA-5151-01</th>\n","      <th>TCGA-BA-5152-01</th>\n","      <th>TCGA-BA-5153-01</th>\n","      <th>TCGA-BA-5555-01</th>\n","      <th>TCGA-BA-5556-01</th>\n","      <th>TCGA-BA-5557-01</th>\n","      <th>TCGA-BA-5558-01</th>\n","      <th>TCGA-BA-5559-01</th>\n","      <th>TCGA-BA-6868-01</th>\n","      <th>TCGA-BA-6869-01</th>\n","      <th>TCGA-BA-6870-01</th>\n","      <th>TCGA-BA-6871-01</th>\n","      <th>TCGA-BA-6872-01</th>\n","      <th>TCGA-BA-6873-01</th>\n","      <th>TCGA-BA-7269-01</th>\n","      <th>TCGA-BA-A4IF-01</th>\n","      <th>TCGA-BA-A4IG-01</th>\n","      <th>TCGA-BA-A4IH-01</th>\n","      <th>TCGA-BA-A4II-01</th>\n","      <th>TCGA-BA-A6D8-01</th>\n","      <th>TCGA-BA-A6DA-01</th>\n","      <th>TCGA-BA-A6DB-01</th>\n","      <th>TCGA-BA-A6DD-01</th>\n","      <th>TCGA-BA-A6DE-01</th>\n","      <th>TCGA-BA-A6DF-01</th>\n","      <th>TCGA-BA-A6DG-01</th>\n","      <th>TCGA-BA-A6DI-01</th>\n","      <th>TCGA-BA-A6DJ-01</th>\n","      <th>TCGA-BA-A6DL-01</th>\n","      <th>...</th>\n","      <th>TCGA-QK-A8Z7-01</th>\n","      <th>TCGA-QK-A8Z8-01</th>\n","      <th>TCGA-QK-A8Z9-01</th>\n","      <th>TCGA-QK-A8ZA-01</th>\n","      <th>TCGA-QK-A8ZB-01</th>\n","      <th>TCGA-QK-AA3J-01</th>\n","      <th>TCGA-QK-AA3K-01</th>\n","      <th>TCGA-RS-A6TO-01</th>\n","      <th>TCGA-RS-A6TP-01</th>\n","      <th>TCGA-T2-A6WX-01</th>\n","      <th>TCGA-T2-A6WZ-01</th>\n","      <th>TCGA-T2-A6X0-01</th>\n","      <th>TCGA-T2-A6X2-01</th>\n","      <th>TCGA-T3-A92M-01</th>\n","      <th>TCGA-T3-A92N-01</th>\n","      <th>TCGA-TN-A7HI-01</th>\n","      <th>TCGA-TN-A7HJ-01</th>\n","      <th>TCGA-TN-A7HL-01</th>\n","      <th>TCGA-UF-A718-01</th>\n","      <th>TCGA-UF-A719-01</th>\n","      <th>TCGA-UF-A71A-06</th>\n","      <th>TCGA-UF-A71A-01</th>\n","      <th>TCGA-UF-A71B-01</th>\n","      <th>TCGA-UF-A71D-01</th>\n","      <th>TCGA-UF-A71E-01</th>\n","      <th>TCGA-UF-A7J9-01</th>\n","      <th>TCGA-UF-A7JA-01</th>\n","      <th>TCGA-UF-A7JC-01</th>\n","      <th>TCGA-UF-A7JD-01</th>\n","      <th>TCGA-UF-A7JF-01</th>\n","      <th>TCGA-UF-A7JH-01</th>\n","      <th>TCGA-UF-A7JJ-01</th>\n","      <th>TCGA-UF-A7JK-01</th>\n","      <th>TCGA-UF-A7JO-01</th>\n","      <th>TCGA-UF-A7JS-01</th>\n","      <th>TCGA-UF-A7JT-01</th>\n","      <th>TCGA-UF-A7JV-01</th>\n","      <th>TCGA-UP-A6WW-01</th>\n","      <th>TCGA-WA-A7GZ-01</th>\n","      <th>TCGA-WA-A7H4-01</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>14686</td>\n","      <td>GSTM1</td>\n","      <td>2944.0</td>\n","      <td>0.119655</td>\n","      <td>0.036893</td>\n","      <td>0.031900</td>\n","      <td>0.178476</td>\n","      <td>0.937528</td>\n","      <td>0.884739</td>\n","      <td>0.185403</td>\n","      <td>0.063763</td>\n","      <td>0.829054</td>\n","      <td>0.151500</td>\n","      <td>0.927675</td>\n","      <td>0.898993</td>\n","      <td>0.025696</td>\n","      <td>0.067859</td>\n","      <td>0.692876</td>\n","      <td>0.724886</td>\n","      <td>0.144395</td>\n","      <td>0.259713</td>\n","      <td>0.287737</td>\n","      <td>0.349038</td>\n","      <td>0.324864</td>\n","      <td>0.923756</td>\n","      <td>0.705415</td>\n","      <td>0.766223</td>\n","      <td>0.743542</td>\n","      <td>0.903797</td>\n","      <td>0.709285</td>\n","      <td>0.178395</td>\n","      <td>0.870161</td>\n","      <td>0.613987</td>\n","      <td>0.062842</td>\n","      <td>0.047501</td>\n","      <td>0.088807</td>\n","      <td>0.042666</td>\n","      <td>0.616226</td>\n","      <td>0.880707</td>\n","      <td>0.799584</td>\n","      <td>...</td>\n","      <td>0.108018</td>\n","      <td>0.051122</td>\n","      <td>0.120006</td>\n","      <td>0.119649</td>\n","      <td>0.052534</td>\n","      <td>0.147349</td>\n","      <td>0.055442</td>\n","      <td>0.029364</td>\n","      <td>0.119017</td>\n","      <td>0.125712</td>\n","      <td>0.064903</td>\n","      <td>0.151956</td>\n","      <td>0.113249</td>\n","      <td>0.077113</td>\n","      <td>0.836390</td>\n","      <td>0.163229</td>\n","      <td>0.659147</td>\n","      <td>0.085788</td>\n","      <td>0.042273</td>\n","      <td>0.264163</td>\n","      <td>0.887039</td>\n","      <td>0.721269</td>\n","      <td>0.056660</td>\n","      <td>0.023511</td>\n","      <td>0.121055</td>\n","      <td>0.111313</td>\n","      <td>0.561632</td>\n","      <td>0.067609</td>\n","      <td>0.030788</td>\n","      <td>0.139835</td>\n","      <td>0.079715</td>\n","      <td>0.704300</td>\n","      <td>0.126556</td>\n","      <td>0.818535</td>\n","      <td>0.854941</td>\n","      <td>0.137747</td>\n","      <td>0.767590</td>\n","      <td>0.153425</td>\n","      <td>0.356625</td>\n","      <td>0.036943</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2347</td>\n","      <td>MYH14</td>\n","      <td>79784.0</td>\n","      <td>0.099371</td>\n","      <td>0.186882</td>\n","      <td>0.751656</td>\n","      <td>0.857746</td>\n","      <td>0.934318</td>\n","      <td>0.091442</td>\n","      <td>0.061266</td>\n","      <td>0.803596</td>\n","      <td>0.683065</td>\n","      <td>0.673634</td>\n","      <td>0.089523</td>\n","      <td>0.116807</td>\n","      <td>0.732102</td>\n","      <td>0.528028</td>\n","      <td>0.834650</td>\n","      <td>0.392971</td>\n","      <td>0.365760</td>\n","      <td>0.077893</td>\n","      <td>0.397707</td>\n","      <td>0.783098</td>\n","      <td>0.874083</td>\n","      <td>0.820718</td>\n","      <td>0.063372</td>\n","      <td>0.217599</td>\n","      <td>0.155897</td>\n","      <td>0.106839</td>\n","      <td>0.322337</td>\n","      <td>0.075381</td>\n","      <td>0.493337</td>\n","      <td>0.612363</td>\n","      <td>0.526420</td>\n","      <td>0.904590</td>\n","      <td>0.828589</td>\n","      <td>0.670581</td>\n","      <td>0.582012</td>\n","      <td>0.909510</td>\n","      <td>0.769562</td>\n","      <td>...</td>\n","      <td>0.072353</td>\n","      <td>0.083179</td>\n","      <td>0.812575</td>\n","      <td>0.585740</td>\n","      <td>0.359282</td>\n","      <td>0.071034</td>\n","      <td>0.510412</td>\n","      <td>0.772016</td>\n","      <td>0.061551</td>\n","      <td>0.893538</td>\n","      <td>0.620299</td>\n","      <td>0.109100</td>\n","      <td>0.866219</td>\n","      <td>0.595384</td>\n","      <td>0.915419</td>\n","      <td>0.101443</td>\n","      <td>0.659007</td>\n","      <td>0.468878</td>\n","      <td>0.144673</td>\n","      <td>0.111141</td>\n","      <td>0.084168</td>\n","      <td>0.061907</td>\n","      <td>0.893554</td>\n","      <td>0.489704</td>\n","      <td>0.438504</td>\n","      <td>0.069853</td>\n","      <td>0.335069</td>\n","      <td>0.894962</td>\n","      <td>0.193605</td>\n","      <td>0.090313</td>\n","      <td>0.073892</td>\n","      <td>0.589182</td>\n","      <td>0.897440</td>\n","      <td>0.896961</td>\n","      <td>0.826484</td>\n","      <td>0.806370</td>\n","      <td>0.351661</td>\n","      <td>0.713009</td>\n","      <td>0.820614</td>\n","      <td>0.200124</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>9037</td>\n","      <td>AEBP1</td>\n","      <td>165.0</td>\n","      <td>0.084202</td>\n","      <td>0.022080</td>\n","      <td>0.266477</td>\n","      <td>0.750876</td>\n","      <td>0.900528</td>\n","      <td>0.028733</td>\n","      <td>0.019199</td>\n","      <td>0.711596</td>\n","      <td>0.569134</td>\n","      <td>0.678753</td>\n","      <td>0.019379</td>\n","      <td>0.033204</td>\n","      <td>0.496102</td>\n","      <td>0.505320</td>\n","      <td>0.587426</td>\n","      <td>0.312002</td>\n","      <td>0.595097</td>\n","      <td>0.050103</td>\n","      <td>0.614568</td>\n","      <td>0.497328</td>\n","      <td>0.250098</td>\n","      <td>0.472463</td>\n","      <td>0.016287</td>\n","      <td>0.030045</td>\n","      <td>0.040408</td>\n","      <td>0.020750</td>\n","      <td>0.027558</td>\n","      <td>0.019972</td>\n","      <td>0.605917</td>\n","      <td>0.032864</td>\n","      <td>0.230782</td>\n","      <td>0.032584</td>\n","      <td>0.690168</td>\n","      <td>0.313193</td>\n","      <td>0.078228</td>\n","      <td>0.701594</td>\n","      <td>0.024534</td>\n","      <td>...</td>\n","      <td>0.018786</td>\n","      <td>0.027948</td>\n","      <td>0.661648</td>\n","      <td>0.019157</td>\n","      <td>0.029847</td>\n","      <td>0.063333</td>\n","      <td>0.019280</td>\n","      <td>0.535348</td>\n","      <td>0.019841</td>\n","      <td>0.664825</td>\n","      <td>0.705036</td>\n","      <td>0.067354</td>\n","      <td>0.763147</td>\n","      <td>0.026202</td>\n","      <td>0.784555</td>\n","      <td>0.023009</td>\n","      <td>0.649773</td>\n","      <td>0.021435</td>\n","      <td>0.519956</td>\n","      <td>0.021525</td>\n","      <td>0.024487</td>\n","      <td>0.027272</td>\n","      <td>0.226416</td>\n","      <td>0.752458</td>\n","      <td>0.813727</td>\n","      <td>0.023402</td>\n","      <td>0.018830</td>\n","      <td>0.834155</td>\n","      <td>0.030919</td>\n","      <td>0.240368</td>\n","      <td>0.022070</td>\n","      <td>0.387631</td>\n","      <td>0.681383</td>\n","      <td>0.573266</td>\n","      <td>0.725304</td>\n","      <td>0.591735</td>\n","      <td>0.480374</td>\n","      <td>0.031688</td>\n","      <td>0.702227</td>\n","      <td>0.426351</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1800</td>\n","      <td>WWOX</td>\n","      <td>51741.0</td>\n","      <td>0.084108</td>\n","      <td>0.876060</td>\n","      <td>0.340502</td>\n","      <td>0.569547</td>\n","      <td>0.941361</td>\n","      <td>0.190372</td>\n","      <td>0.151340</td>\n","      <td>0.610040</td>\n","      <td>0.947681</td>\n","      <td>0.850265</td>\n","      <td>0.301977</td>\n","      <td>0.235201</td>\n","      <td>0.945708</td>\n","      <td>0.887447</td>\n","      <td>0.939635</td>\n","      <td>0.259108</td>\n","      <td>0.120829</td>\n","      <td>0.712123</td>\n","      <td>0.900227</td>\n","      <td>0.087304</td>\n","      <td>0.911047</td>\n","      <td>0.902415</td>\n","      <td>0.220637</td>\n","      <td>0.775412</td>\n","      <td>0.732739</td>\n","      <td>0.106195</td>\n","      <td>0.875589</td>\n","      <td>0.847699</td>\n","      <td>0.894700</td>\n","      <td>0.941389</td>\n","      <td>0.888654</td>\n","      <td>0.907597</td>\n","      <td>0.927428</td>\n","      <td>0.908435</td>\n","      <td>0.741015</td>\n","      <td>0.939456</td>\n","      <td>0.958829</td>\n","      <td>...</td>\n","      <td>0.844879</td>\n","      <td>0.105583</td>\n","      <td>0.875782</td>\n","      <td>0.754929</td>\n","      <td>0.272306</td>\n","      <td>0.075514</td>\n","      <td>0.537663</td>\n","      <td>0.854144</td>\n","      <td>0.935852</td>\n","      <td>0.950850</td>\n","      <td>0.955799</td>\n","      <td>0.527957</td>\n","      <td>0.932417</td>\n","      <td>0.824920</td>\n","      <td>0.802431</td>\n","      <td>0.830407</td>\n","      <td>0.613233</td>\n","      <td>0.374753</td>\n","      <td>0.950802</td>\n","      <td>0.509344</td>\n","      <td>0.620985</td>\n","      <td>0.885983</td>\n","      <td>0.693862</td>\n","      <td>0.406001</td>\n","      <td>0.906037</td>\n","      <td>0.487856</td>\n","      <td>0.411547</td>\n","      <td>0.679116</td>\n","      <td>0.298215</td>\n","      <td>0.224900</td>\n","      <td>0.327966</td>\n","      <td>0.942308</td>\n","      <td>0.798592</td>\n","      <td>0.965352</td>\n","      <td>0.900028</td>\n","      <td>0.963398</td>\n","      <td>0.927480</td>\n","      <td>0.218201</td>\n","      <td>0.279617</td>\n","      <td>0.767813</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2563</td>\n","      <td>ZNF167</td>\n","      <td>55888.0</td>\n","      <td>0.083289</td>\n","      <td>0.135661</td>\n","      <td>0.515617</td>\n","      <td>0.715852</td>\n","      <td>0.884980</td>\n","      <td>0.129125</td>\n","      <td>0.084583</td>\n","      <td>0.405630</td>\n","      <td>0.756499</td>\n","      <td>0.120982</td>\n","      <td>0.903724</td>\n","      <td>0.084541</td>\n","      <td>0.637652</td>\n","      <td>0.379267</td>\n","      <td>0.245877</td>\n","      <td>0.886830</td>\n","      <td>0.057867</td>\n","      <td>0.071110</td>\n","      <td>0.124103</td>\n","      <td>0.853076</td>\n","      <td>0.540508</td>\n","      <td>0.696283</td>\n","      <td>0.076439</td>\n","      <td>0.093955</td>\n","      <td>0.351007</td>\n","      <td>0.151349</td>\n","      <td>0.232290</td>\n","      <td>0.058878</td>\n","      <td>0.488093</td>\n","      <td>0.094837</td>\n","      <td>0.439715</td>\n","      <td>0.075053</td>\n","      <td>0.758620</td>\n","      <td>0.363106</td>\n","      <td>0.369338</td>\n","      <td>0.827154</td>\n","      <td>0.085254</td>\n","      <td>...</td>\n","      <td>0.047411</td>\n","      <td>0.076243</td>\n","      <td>0.633744</td>\n","      <td>0.137585</td>\n","      <td>0.127153</td>\n","      <td>0.783122</td>\n","      <td>0.044589</td>\n","      <td>0.208750</td>\n","      <td>0.957828</td>\n","      <td>0.824296</td>\n","      <td>0.717970</td>\n","      <td>0.859705</td>\n","      <td>0.779289</td>\n","      <td>0.158903</td>\n","      <td>0.878923</td>\n","      <td>0.949226</td>\n","      <td>0.375769</td>\n","      <td>0.322575</td>\n","      <td>0.324457</td>\n","      <td>0.104114</td>\n","      <td>0.811218</td>\n","      <td>0.878127</td>\n","      <td>0.095893</td>\n","      <td>0.053895</td>\n","      <td>0.865552</td>\n","      <td>0.073243</td>\n","      <td>0.066255</td>\n","      <td>0.059344</td>\n","      <td>0.058545</td>\n","      <td>0.076170</td>\n","      <td>0.067012</td>\n","      <td>0.177303</td>\n","      <td>0.550804</td>\n","      <td>0.742525</td>\n","      <td>0.681371</td>\n","      <td>0.790943</td>\n","      <td>0.098108</td>\n","      <td>0.884296</td>\n","      <td>0.131715</td>\n","      <td>0.186893</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 534 columns</p>\n","</div>"],"text/plain":["   Unnamed: 0 Hugo_Symbol  ...  TCGA-WA-A7GZ-01  TCGA-WA-A7H4-01\n","0       14686       GSTM1  ...         0.356625         0.036943\n","1        2347       MYH14  ...         0.820614         0.200124\n","2        9037       AEBP1  ...         0.702227         0.426351\n","3        1800        WWOX  ...         0.279617         0.767813\n","4        2563      ZNF167  ...         0.131715         0.186893\n","\n","[5 rows x 534 columns]"]},"metadata":{"tags":[]},"execution_count":150}]},{"cell_type":"code","metadata":{"id":"SSq3_Ao-ToDh"},"source":["meth_var_genes = meth_df.iloc[:400,1]\n","meth_proc_df = meth_df[meth_df.Hugo_Symbol.isin(meth_var_genes)]\n","meth_proc_df = meth_proc_df.fillna(0)\n","\n","X = meth_proc_df[[pat + \"-01\" for pat in patients_df.PATIENT_ID]].T\n","y = patients_df['BINARY_SURVIVAL']\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4MMg12WTYegV"},"source":["### Survival Months Not Available for these two patients ####\n","pats_df = patients_df[~patients_df.PATIENT_ID.isin(['TCGA-CQ-A4CA','TCGA-H7-A6C4'])]\n","X_lreg = meth_proc_df[[pat + \"-01\" for pat in pats_df.PATIENT_ID]].T\n","patients_survival = y_lreg = pats_df['OS_MONTHS']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a4LKjLWpXLkk"},"source":["## Models"]},{"cell_type":"markdown","metadata":{"id":"GRevbd01XZLW"},"source":["**Binary Classification**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pCkd4UUsXO9Y","executionInfo":{"status":"ok","timestamp":1620498390683,"user_tz":240,"elapsed":5212,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"8015014d-1efe-41bc-e45e-55354340dd53"},"source":["clf = RandomForestClassifier()\n","cv_results = cross_validate(clf, X, y, cv=5, scoring='roc_auc')\n","print(\"RFC\", np.mean(cv_results['test_score']))\n","\n","#try l1 regularization \n","clf = LogisticRegression(penalty='l1', solver='liblinear', C=1, class_weight='balanced')\n","cv_results = cross_validate(clf, X, y, cv=5, scoring='roc_auc')\n","print(\"LR\", np.mean(cv_results['test_score']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["RFC 0.5910281503560192\n","LR 0.5809606446983496\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Aov-Fq9vX29E"},"source":["**Results**\n","\n","400: \\\\\n","59 | 57 or 59 | 58\n","\n","200: \\\\\n","56 | 55\n","\n","100: \\\\\n","57 | 58 or 59 | 58\n","\n","50: \\\\\n","60 | 56\n","\n","25: \\\\\n","57 | 54\n","\n","10: \\\\\n","53 | 52\n"]},{"cell_type":"markdown","metadata":{"id":"gE6LWDgXX5DM"},"source":["**Linear Regression**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gLOQpGAbX6xs","executionInfo":{"status":"ok","timestamp":1620251691356,"user_tz":240,"elapsed":241,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"1514d4b3-b5f8-4fa2-f3f2-103246fb4401"},"source":["lreg = LinearRegression()\n","cv_results = cross_validate(lreg, X_lreg, y_lreg, cv=5, return_train_score = True)\n","print(\"LinReg\", np.mean(cv_results['test_score']))z"],"execution_count":null,"outputs":[{"output_type":"stream","text":["LinReg -0.13311600494873105\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ykbE0v8TX7RS"},"source":["** *italicized text*Results**\n","400: -139.7 \\\\\n","200: -1.93 \\\\\n","100: -0.66 \\\\\n","50: -0.30 \\\\\n","25: -0.17 \\\\\n","10: -0.13 \\\\"]},{"cell_type":"markdown","metadata":{"id":"Jx3Z7sHGnRrn"},"source":["# Meth and RNA overlap"]},{"cell_type":"markdown","metadata":{"id":"kKlJew6Ln-v_"},"source":["## Load"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"ZxkFY8BCxmY6","executionInfo":{"status":"ok","timestamp":1621021359917,"user_tz":240,"elapsed":104975,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"37ccd345-76d2-40fe-84ee-c351a555f65b"},"source":["clean_path = Path(eileen_path + \"cleaned_clinical_data.csv\")\n","clean_df = pd.read_csv(clean_path)\n","\n","patients_df = clean_df[['PATIENT_ID', 'OS_MONTHS', 'BINARY_SURVIVAL']]\n","patients_df = patients_df[~patients_df.PATIENT_ID.isin( ['TCGA-MT-A67G', 'TCGA-CQ-7064', 'TCGA-IQ-A61L', 'TCGA-CN-A63Y', 'TCGA-BA-A6DF', 'TCGA-IQ-A61K', 'TCGA-CN-A640'])]\n","\n","rna_path = Path(proj_path + 'normalized-rna-data.xlsx')\n","rna_df = pd.read_excel(rna_path)\n","\n","meth_path = Path(sina_path + \"top1000var_meth.csv\")\n","meth_df = pd.read_csv(meth_path)\n","meth_df = meth_df.fillna(0)\n","\n","\n","genes_path = Path(sina_path + \"overlap_sina_joseph.csv\")\n","genes_df = pd.read_csv(genes_path)\n","genes_df.head()"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>gene</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>CLDN17</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>ALX1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>MRAP2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>LOC254559</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>SLC13A5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0       gene\n","0           0     CLDN17\n","1           1       ALX1\n","2           2      MRAP2\n","3           3  LOC254559\n","4           4    SLC13A5"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":473},"id":"SjLFJFgdZbp-","executionInfo":{"status":"ok","timestamp":1621021360503,"user_tz":240,"elapsed":579,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"93b15c0e-440f-4ed6-96d3-f83911f78a4d"},"source":["rna_df"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Gene</th>\n","      <th>TCGA-4P-AA8J</th>\n","      <th>TCGA-BA-4074</th>\n","      <th>TCGA-BA-4075</th>\n","      <th>TCGA-BA-4076</th>\n","      <th>TCGA-BA-4077</th>\n","      <th>TCGA-BA-4078</th>\n","      <th>TCGA-BA-5149</th>\n","      <th>TCGA-BA-5151</th>\n","      <th>TCGA-BA-5152</th>\n","      <th>TCGA-BA-5153</th>\n","      <th>TCGA-BA-5555</th>\n","      <th>TCGA-BA-5556</th>\n","      <th>TCGA-BA-5557</th>\n","      <th>TCGA-BA-5558</th>\n","      <th>TCGA-BA-5559</th>\n","      <th>TCGA-BA-6868</th>\n","      <th>TCGA-BA-6869</th>\n","      <th>TCGA-BA-6870</th>\n","      <th>TCGA-BA-6871</th>\n","      <th>TCGA-BA-6872</th>\n","      <th>TCGA-BA-6873</th>\n","      <th>TCGA-BA-7269</th>\n","      <th>TCGA-BA-A4IF</th>\n","      <th>TCGA-BA-A4IG</th>\n","      <th>TCGA-BA-A4IH</th>\n","      <th>TCGA-BA-A4II</th>\n","      <th>TCGA-BA-A6D8</th>\n","      <th>TCGA-BA-A6DA</th>\n","      <th>TCGA-BA-A6DB</th>\n","      <th>TCGA-BA-A6DD</th>\n","      <th>TCGA-BA-A6DE</th>\n","      <th>TCGA-BA-A6DG</th>\n","      <th>TCGA-BA-A6DI</th>\n","      <th>TCGA-BA-A6DJ</th>\n","      <th>TCGA-BA-A6DL</th>\n","      <th>TCGA-BA-A8YP</th>\n","      <th>TCGA-BB-4217</th>\n","      <th>TCGA-BB-4223</th>\n","      <th>TCGA-BB-4224</th>\n","      <th>...</th>\n","      <th>TCGA-QK-A8Z8</th>\n","      <th>TCGA-QK-A8Z9</th>\n","      <th>TCGA-QK-A8ZA</th>\n","      <th>TCGA-QK-A8ZB</th>\n","      <th>TCGA-QK-AA3J</th>\n","      <th>TCGA-QK-AA3K</th>\n","      <th>TCGA-RS-A6TO</th>\n","      <th>TCGA-RS-A6TP</th>\n","      <th>TCGA-T2-A6WX</th>\n","      <th>TCGA-T2-A6WZ</th>\n","      <th>TCGA-T2-A6X0</th>\n","      <th>TCGA-T2-A6X2</th>\n","      <th>TCGA-T3-A92M</th>\n","      <th>TCGA-T3-A92N</th>\n","      <th>TCGA-TN-A7HI</th>\n","      <th>TCGA-TN-A7HJ</th>\n","      <th>TCGA-TN-A7HL</th>\n","      <th>TCGA-UF-A718</th>\n","      <th>TCGA-UF-A719</th>\n","      <th>TCGA-UF-A71A</th>\n","      <th>TCGA-UF-A71A.1</th>\n","      <th>TCGA-UF-A71B</th>\n","      <th>TCGA-UF-A71D</th>\n","      <th>TCGA-UF-A71E</th>\n","      <th>TCGA-UF-A7J9</th>\n","      <th>TCGA-UF-A7JA</th>\n","      <th>TCGA-UF-A7JC</th>\n","      <th>TCGA-UF-A7JD</th>\n","      <th>TCGA-UF-A7JF</th>\n","      <th>TCGA-UF-A7JH</th>\n","      <th>TCGA-UF-A7JJ</th>\n","      <th>TCGA-UF-A7JK</th>\n","      <th>TCGA-UF-A7JO</th>\n","      <th>TCGA-UF-A7JS</th>\n","      <th>TCGA-UF-A7JT</th>\n","      <th>TCGA-UF-A7JV</th>\n","      <th>TCGA-UP-A6WW</th>\n","      <th>TCGA-WA-A7GZ</th>\n","      <th>TCGA-WA-A7GZ.1</th>\n","      <th>TCGA-WA-A7H4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>?|100133144</td>\n","      <td>-2.668367</td>\n","      <td>-0.902774</td>\n","      <td>-0.875868</td>\n","      <td>-0.788130</td>\n","      <td>-0.720443</td>\n","      <td>0.305505</td>\n","      <td>-1.884686</td>\n","      <td>-2.273196</td>\n","      <td>-1.028562</td>\n","      <td>-2.359103</td>\n","      <td>-2.267961</td>\n","      <td>-1.734984</td>\n","      <td>-2.292535</td>\n","      <td>-2.034867</td>\n","      <td>-1.445529</td>\n","      <td>-1.107918</td>\n","      <td>-2.693288</td>\n","      <td>-4.924138</td>\n","      <td>-0.598996</td>\n","      <td>-2.701728</td>\n","      <td>-1.693326</td>\n","      <td>-1.028824</td>\n","      <td>-1.126058</td>\n","      <td>-0.177138</td>\n","      <td>0.555868</td>\n","      <td>-4.038260</td>\n","      <td>-0.418175</td>\n","      <td>-0.350013</td>\n","      <td>-3.113069</td>\n","      <td>-1.636349</td>\n","      <td>-1.669245</td>\n","      <td>-1.396426</td>\n","      <td>-0.250961</td>\n","      <td>-0.698789</td>\n","      <td>-1.235164</td>\n","      <td>-1.715348</td>\n","      <td>-2.172558</td>\n","      <td>0.692689</td>\n","      <td>-0.759041</td>\n","      <td>...</td>\n","      <td>-3.655830</td>\n","      <td>-2.671399</td>\n","      <td>-1.226739</td>\n","      <td>0.080303</td>\n","      <td>0.097108</td>\n","      <td>-2.735148</td>\n","      <td>-0.282547</td>\n","      <td>0.766106</td>\n","      <td>-0.510861</td>\n","      <td>0.365926</td>\n","      <td>-0.063281</td>\n","      <td>-0.349853</td>\n","      <td>-1.191962</td>\n","      <td>-1.413758</td>\n","      <td>-2.039966</td>\n","      <td>0.450061</td>\n","      <td>-2.250369</td>\n","      <td>-0.576201</td>\n","      <td>-0.892550</td>\n","      <td>0.412633</td>\n","      <td>-0.375885</td>\n","      <td>-0.002416</td>\n","      <td>0.012172</td>\n","      <td>-0.885552</td>\n","      <td>0.107522</td>\n","      <td>0.186019</td>\n","      <td>0.248591</td>\n","      <td>-0.595343</td>\n","      <td>-1.099779</td>\n","      <td>-0.509501</td>\n","      <td>-1.487508</td>\n","      <td>0.784834</td>\n","      <td>-1.195505</td>\n","      <td>-2.153869</td>\n","      <td>-2.552769</td>\n","      <td>-2.570372</td>\n","      <td>-0.957308</td>\n","      <td>1.103520</td>\n","      <td>1.055458</td>\n","      <td>0.110682</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>?|100134869</td>\n","      <td>-0.794596</td>\n","      <td>-1.020686</td>\n","      <td>-2.464098</td>\n","      <td>-2.221278</td>\n","      <td>-0.817997</td>\n","      <td>-0.380211</td>\n","      <td>-1.300005</td>\n","      <td>-2.467344</td>\n","      <td>-2.103687</td>\n","      <td>-1.200386</td>\n","      <td>-2.639825</td>\n","      <td>-1.386775</td>\n","      <td>-1.542003</td>\n","      <td>-0.099920</td>\n","      <td>-1.203517</td>\n","      <td>-0.184573</td>\n","      <td>-2.223283</td>\n","      <td>-1.524609</td>\n","      <td>0.511413</td>\n","      <td>-1.497810</td>\n","      <td>-0.519020</td>\n","      <td>-2.926866</td>\n","      <td>0.069059</td>\n","      <td>-0.617449</td>\n","      <td>0.294437</td>\n","      <td>-1.599741</td>\n","      <td>-1.268774</td>\n","      <td>-2.703403</td>\n","      <td>-1.258081</td>\n","      <td>-1.987165</td>\n","      <td>-2.019275</td>\n","      <td>-4.073444</td>\n","      <td>-0.064970</td>\n","      <td>-1.485542</td>\n","      <td>-1.667472</td>\n","      <td>-0.351495</td>\n","      <td>-1.863715</td>\n","      <td>0.325171</td>\n","      <td>-1.096923</td>\n","      <td>...</td>\n","      <td>-2.005872</td>\n","      <td>-1.121858</td>\n","      <td>-0.589878</td>\n","      <td>0.244041</td>\n","      <td>0.619169</td>\n","      <td>-1.347078</td>\n","      <td>-1.228849</td>\n","      <td>1.150059</td>\n","      <td>-0.165324</td>\n","      <td>-1.335510</td>\n","      <td>1.230945</td>\n","      <td>0.076515</td>\n","      <td>-0.698765</td>\n","      <td>-0.998200</td>\n","      <td>-1.801836</td>\n","      <td>0.926873</td>\n","      <td>-1.446833</td>\n","      <td>-0.321043</td>\n","      <td>-0.846905</td>\n","      <td>1.036154</td>\n","      <td>-0.526208</td>\n","      <td>-0.385914</td>\n","      <td>-1.147425</td>\n","      <td>-0.536055</td>\n","      <td>0.258005</td>\n","      <td>0.332227</td>\n","      <td>0.173918</td>\n","      <td>-1.749557</td>\n","      <td>-0.305143</td>\n","      <td>-1.010059</td>\n","      <td>-0.272792</td>\n","      <td>0.844650</td>\n","      <td>-1.583013</td>\n","      <td>-2.747920</td>\n","      <td>-3.398868</td>\n","      <td>-0.710990</td>\n","      <td>-0.290401</td>\n","      <td>1.118641</td>\n","      <td>0.730860</td>\n","      <td>0.065397</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>?|10357</td>\n","      <td>2.411792</td>\n","      <td>3.752946</td>\n","      <td>3.888404</td>\n","      <td>3.634904</td>\n","      <td>3.915315</td>\n","      <td>3.313590</td>\n","      <td>3.563117</td>\n","      <td>2.727658</td>\n","      <td>2.735800</td>\n","      <td>3.036732</td>\n","      <td>2.753468</td>\n","      <td>2.934567</td>\n","      <td>2.452501</td>\n","      <td>3.283812</td>\n","      <td>3.655104</td>\n","      <td>3.285483</td>\n","      <td>1.962820</td>\n","      <td>2.940204</td>\n","      <td>3.198316</td>\n","      <td>2.351329</td>\n","      <td>3.338756</td>\n","      <td>1.387340</td>\n","      <td>4.097065</td>\n","      <td>3.339682</td>\n","      <td>3.665524</td>\n","      <td>2.800231</td>\n","      <td>1.799342</td>\n","      <td>0.556048</td>\n","      <td>2.536214</td>\n","      <td>2.694082</td>\n","      <td>2.321353</td>\n","      <td>2.114732</td>\n","      <td>2.570874</td>\n","      <td>2.725813</td>\n","      <td>2.521753</td>\n","      <td>3.613625</td>\n","      <td>4.021507</td>\n","      <td>3.273238</td>\n","      <td>3.727679</td>\n","      <td>...</td>\n","      <td>2.515806</td>\n","      <td>1.929937</td>\n","      <td>3.203928</td>\n","      <td>1.798339</td>\n","      <td>3.225687</td>\n","      <td>2.533997</td>\n","      <td>0.494128</td>\n","      <td>3.026564</td>\n","      <td>2.555471</td>\n","      <td>1.452136</td>\n","      <td>2.279025</td>\n","      <td>2.184747</td>\n","      <td>3.471884</td>\n","      <td>3.265413</td>\n","      <td>2.644048</td>\n","      <td>2.297885</td>\n","      <td>2.424818</td>\n","      <td>1.051801</td>\n","      <td>2.293206</td>\n","      <td>4.051905</td>\n","      <td>3.576949</td>\n","      <td>2.762821</td>\n","      <td>1.628951</td>\n","      <td>2.265648</td>\n","      <td>2.014319</td>\n","      <td>2.358678</td>\n","      <td>1.522075</td>\n","      <td>2.625551</td>\n","      <td>2.010490</td>\n","      <td>2.429561</td>\n","      <td>1.744007</td>\n","      <td>2.406473</td>\n","      <td>3.157449</td>\n","      <td>-0.420827</td>\n","      <td>2.519974</td>\n","      <td>2.545822</td>\n","      <td>2.312506</td>\n","      <td>1.767151</td>\n","      <td>2.989047</td>\n","      <td>2.724889</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>?|10431</td>\n","      <td>5.945483</td>\n","      <td>6.270801</td>\n","      <td>5.988357</td>\n","      <td>5.749688</td>\n","      <td>5.322637</td>\n","      <td>5.071589</td>\n","      <td>5.728863</td>\n","      <td>6.196538</td>\n","      <td>5.694105</td>\n","      <td>5.245816</td>\n","      <td>4.967065</td>\n","      <td>5.191896</td>\n","      <td>4.908296</td>\n","      <td>5.308824</td>\n","      <td>5.207073</td>\n","      <td>5.195706</td>\n","      <td>5.975070</td>\n","      <td>5.399957</td>\n","      <td>5.708008</td>\n","      <td>5.480605</td>\n","      <td>5.582467</td>\n","      <td>6.821470</td>\n","      <td>6.130176</td>\n","      <td>5.893784</td>\n","      <td>5.690444</td>\n","      <td>5.834212</td>\n","      <td>6.357782</td>\n","      <td>5.306547</td>\n","      <td>5.648564</td>\n","      <td>6.023422</td>\n","      <td>6.712605</td>\n","      <td>5.167052</td>\n","      <td>5.104636</td>\n","      <td>5.603932</td>\n","      <td>6.236621</td>\n","      <td>4.851152</td>\n","      <td>5.798102</td>\n","      <td>5.261801</td>\n","      <td>6.403219</td>\n","      <td>...</td>\n","      <td>5.758368</td>\n","      <td>5.037845</td>\n","      <td>5.685049</td>\n","      <td>5.487995</td>\n","      <td>5.533157</td>\n","      <td>5.522161</td>\n","      <td>5.838139</td>\n","      <td>5.418221</td>\n","      <td>5.614866</td>\n","      <td>6.652706</td>\n","      <td>5.481120</td>\n","      <td>5.563760</td>\n","      <td>5.564250</td>\n","      <td>5.871047</td>\n","      <td>6.304907</td>\n","      <td>5.586194</td>\n","      <td>5.982780</td>\n","      <td>4.730400</td>\n","      <td>5.587638</td>\n","      <td>5.466640</td>\n","      <td>5.384323</td>\n","      <td>5.269697</td>\n","      <td>6.112797</td>\n","      <td>5.679450</td>\n","      <td>5.477513</td>\n","      <td>5.304144</td>\n","      <td>6.047472</td>\n","      <td>5.436376</td>\n","      <td>5.963395</td>\n","      <td>5.358525</td>\n","      <td>4.973021</td>\n","      <td>5.648660</td>\n","      <td>5.790883</td>\n","      <td>5.388906</td>\n","      <td>6.208256</td>\n","      <td>5.519225</td>\n","      <td>5.984255</td>\n","      <td>5.102151</td>\n","      <td>4.833012</td>\n","      <td>5.228947</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>?|155060</td>\n","      <td>3.523286</td>\n","      <td>0.672797</td>\n","      <td>1.733573</td>\n","      <td>2.330802</td>\n","      <td>1.187771</td>\n","      <td>3.408542</td>\n","      <td>1.881185</td>\n","      <td>1.889117</td>\n","      <td>-0.802738</td>\n","      <td>2.312371</td>\n","      <td>2.269571</td>\n","      <td>1.538062</td>\n","      <td>-0.186946</td>\n","      <td>2.914725</td>\n","      <td>2.770239</td>\n","      <td>1.205832</td>\n","      <td>2.113614</td>\n","      <td>1.798942</td>\n","      <td>2.500964</td>\n","      <td>1.176436</td>\n","      <td>2.076542</td>\n","      <td>0.313958</td>\n","      <td>2.683034</td>\n","      <td>1.444088</td>\n","      <td>1.862935</td>\n","      <td>2.745887</td>\n","      <td>2.887599</td>\n","      <td>1.424271</td>\n","      <td>1.981907</td>\n","      <td>0.655867</td>\n","      <td>2.114058</td>\n","      <td>2.856064</td>\n","      <td>3.374485</td>\n","      <td>3.844550</td>\n","      <td>2.470323</td>\n","      <td>2.376110</td>\n","      <td>2.829765</td>\n","      <td>3.902923</td>\n","      <td>4.185408</td>\n","      <td>...</td>\n","      <td>3.018713</td>\n","      <td>2.576960</td>\n","      <td>2.783646</td>\n","      <td>1.373680</td>\n","      <td>2.967856</td>\n","      <td>3.295536</td>\n","      <td>4.368545</td>\n","      <td>2.941191</td>\n","      <td>3.719969</td>\n","      <td>2.552855</td>\n","      <td>3.277967</td>\n","      <td>4.265101</td>\n","      <td>3.131309</td>\n","      <td>4.203923</td>\n","      <td>2.422933</td>\n","      <td>3.103904</td>\n","      <td>2.031128</td>\n","      <td>2.620919</td>\n","      <td>2.163159</td>\n","      <td>3.032424</td>\n","      <td>1.620391</td>\n","      <td>3.029272</td>\n","      <td>2.715181</td>\n","      <td>1.928043</td>\n","      <td>2.815603</td>\n","      <td>4.057478</td>\n","      <td>1.809895</td>\n","      <td>3.745528</td>\n","      <td>3.844495</td>\n","      <td>3.879251</td>\n","      <td>2.299783</td>\n","      <td>1.950906</td>\n","      <td>3.446527</td>\n","      <td>1.640209</td>\n","      <td>3.734540</td>\n","      <td>4.292657</td>\n","      <td>2.999944</td>\n","      <td>3.518798</td>\n","      <td>3.219246</td>\n","      <td>3.645937</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>17714</th>\n","      <td>ZYG11B|79699</td>\n","      <td>4.176802</td>\n","      <td>5.011630</td>\n","      <td>4.375613</td>\n","      <td>5.011237</td>\n","      <td>5.175596</td>\n","      <td>5.306228</td>\n","      <td>4.525589</td>\n","      <td>5.531478</td>\n","      <td>4.455235</td>\n","      <td>5.321958</td>\n","      <td>5.232848</td>\n","      <td>4.942224</td>\n","      <td>4.325400</td>\n","      <td>4.618485</td>\n","      <td>5.663775</td>\n","      <td>5.065667</td>\n","      <td>4.735893</td>\n","      <td>4.030061</td>\n","      <td>3.924206</td>\n","      <td>4.348957</td>\n","      <td>4.983306</td>\n","      <td>4.159788</td>\n","      <td>4.801343</td>\n","      <td>5.189473</td>\n","      <td>5.547823</td>\n","      <td>5.450456</td>\n","      <td>3.128834</td>\n","      <td>2.318907</td>\n","      <td>4.480110</td>\n","      <td>4.239696</td>\n","      <td>5.138233</td>\n","      <td>4.642400</td>\n","      <td>4.453986</td>\n","      <td>4.146856</td>\n","      <td>4.528100</td>\n","      <td>5.286468</td>\n","      <td>5.191052</td>\n","      <td>5.581331</td>\n","      <td>5.295726</td>\n","      <td>...</td>\n","      <td>5.272538</td>\n","      <td>5.165469</td>\n","      <td>5.247633</td>\n","      <td>5.143926</td>\n","      <td>5.387656</td>\n","      <td>4.537361</td>\n","      <td>4.370949</td>\n","      <td>5.643979</td>\n","      <td>4.462079</td>\n","      <td>3.984792</td>\n","      <td>5.317850</td>\n","      <td>4.616310</td>\n","      <td>5.034924</td>\n","      <td>4.247831</td>\n","      <td>3.601024</td>\n","      <td>4.676000</td>\n","      <td>4.393398</td>\n","      <td>4.884136</td>\n","      <td>4.710283</td>\n","      <td>4.541032</td>\n","      <td>4.652177</td>\n","      <td>4.826940</td>\n","      <td>5.304197</td>\n","      <td>4.292717</td>\n","      <td>4.923744</td>\n","      <td>5.336817</td>\n","      <td>4.757271</td>\n","      <td>4.733319</td>\n","      <td>4.019582</td>\n","      <td>3.944405</td>\n","      <td>5.019014</td>\n","      <td>4.841961</td>\n","      <td>4.234894</td>\n","      <td>2.858784</td>\n","      <td>3.977971</td>\n","      <td>4.620639</td>\n","      <td>4.268517</td>\n","      <td>6.704048</td>\n","      <td>4.644218</td>\n","      <td>5.545347</td>\n","    </tr>\n","    <tr>\n","      <th>17715</th>\n","      <td>ZYX|7791</td>\n","      <td>8.967494</td>\n","      <td>6.655512</td>\n","      <td>7.978120</td>\n","      <td>6.898629</td>\n","      <td>8.556480</td>\n","      <td>7.625407</td>\n","      <td>7.961349</td>\n","      <td>8.105545</td>\n","      <td>8.007809</td>\n","      <td>8.088375</td>\n","      <td>7.533440</td>\n","      <td>8.375060</td>\n","      <td>8.086689</td>\n","      <td>8.446944</td>\n","      <td>7.866579</td>\n","      <td>7.838962</td>\n","      <td>7.591940</td>\n","      <td>8.039146</td>\n","      <td>7.841152</td>\n","      <td>8.126429</td>\n","      <td>7.886365</td>\n","      <td>7.400414</td>\n","      <td>7.389140</td>\n","      <td>7.088111</td>\n","      <td>6.953599</td>\n","      <td>7.493049</td>\n","      <td>8.184772</td>\n","      <td>6.440854</td>\n","      <td>7.618125</td>\n","      <td>8.054971</td>\n","      <td>6.855054</td>\n","      <td>8.162627</td>\n","      <td>7.434157</td>\n","      <td>8.330301</td>\n","      <td>7.001108</td>\n","      <td>7.497337</td>\n","      <td>7.687370</td>\n","      <td>7.321261</td>\n","      <td>5.846482</td>\n","      <td>...</td>\n","      <td>6.835528</td>\n","      <td>8.056698</td>\n","      <td>7.325141</td>\n","      <td>7.301010</td>\n","      <td>6.187652</td>\n","      <td>8.357988</td>\n","      <td>8.337788</td>\n","      <td>7.541622</td>\n","      <td>8.543962</td>\n","      <td>8.349987</td>\n","      <td>7.400563</td>\n","      <td>8.316960</td>\n","      <td>7.771054</td>\n","      <td>8.555589</td>\n","      <td>7.361930</td>\n","      <td>8.003710</td>\n","      <td>8.429783</td>\n","      <td>7.104674</td>\n","      <td>7.810060</td>\n","      <td>7.598794</td>\n","      <td>8.312401</td>\n","      <td>8.109744</td>\n","      <td>7.298858</td>\n","      <td>7.611575</td>\n","      <td>8.113544</td>\n","      <td>6.950409</td>\n","      <td>7.467648</td>\n","      <td>8.964320</td>\n","      <td>7.395509</td>\n","      <td>7.928419</td>\n","      <td>7.677835</td>\n","      <td>7.804717</td>\n","      <td>7.561616</td>\n","      <td>7.363458</td>\n","      <td>8.530718</td>\n","      <td>8.627586</td>\n","      <td>7.940130</td>\n","      <td>5.879767</td>\n","      <td>7.323343</td>\n","      <td>8.053697</td>\n","    </tr>\n","    <tr>\n","      <th>17716</th>\n","      <td>ZZEF1|23140</td>\n","      <td>5.248309</td>\n","      <td>3.375093</td>\n","      <td>3.639632</td>\n","      <td>4.907054</td>\n","      <td>5.161124</td>\n","      <td>6.302415</td>\n","      <td>5.395046</td>\n","      <td>6.357876</td>\n","      <td>5.312153</td>\n","      <td>6.707029</td>\n","      <td>6.281676</td>\n","      <td>6.547042</td>\n","      <td>5.105842</td>\n","      <td>5.974971</td>\n","      <td>6.993494</td>\n","      <td>5.441161</td>\n","      <td>5.268863</td>\n","      <td>5.477951</td>\n","      <td>5.704279</td>\n","      <td>4.840439</td>\n","      <td>6.235170</td>\n","      <td>4.155221</td>\n","      <td>6.183286</td>\n","      <td>5.489213</td>\n","      <td>4.900039</td>\n","      <td>4.924057</td>\n","      <td>3.780524</td>\n","      <td>4.320263</td>\n","      <td>5.058248</td>\n","      <td>5.101562</td>\n","      <td>5.272685</td>\n","      <td>5.470425</td>\n","      <td>5.958500</td>\n","      <td>5.294936</td>\n","      <td>5.055603</td>\n","      <td>6.218466</td>\n","      <td>6.299420</td>\n","      <td>6.506575</td>\n","      <td>5.359387</td>\n","      <td>...</td>\n","      <td>5.313990</td>\n","      <td>6.229946</td>\n","      <td>5.911126</td>\n","      <td>5.592433</td>\n","      <td>6.605114</td>\n","      <td>5.656951</td>\n","      <td>5.188605</td>\n","      <td>6.802145</td>\n","      <td>5.598902</td>\n","      <td>4.367446</td>\n","      <td>5.623938</td>\n","      <td>5.634493</td>\n","      <td>5.839510</td>\n","      <td>5.017127</td>\n","      <td>4.659010</td>\n","      <td>6.145973</td>\n","      <td>4.481726</td>\n","      <td>5.420508</td>\n","      <td>6.110962</td>\n","      <td>5.324518</td>\n","      <td>5.443304</td>\n","      <td>6.046112</td>\n","      <td>5.369904</td>\n","      <td>5.664159</td>\n","      <td>5.524972</td>\n","      <td>5.894502</td>\n","      <td>6.182935</td>\n","      <td>5.884906</td>\n","      <td>4.961374</td>\n","      <td>5.267850</td>\n","      <td>5.573614</td>\n","      <td>6.203574</td>\n","      <td>5.777273</td>\n","      <td>4.068463</td>\n","      <td>5.406358</td>\n","      <td>4.919382</td>\n","      <td>5.680214</td>\n","      <td>6.206898</td>\n","      <td>6.381286</td>\n","      <td>6.527878</td>\n","    </tr>\n","    <tr>\n","      <th>17717</th>\n","      <td>ZZZ3|26009</td>\n","      <td>3.952576</td>\n","      <td>5.446325</td>\n","      <td>5.381988</td>\n","      <td>5.477708</td>\n","      <td>5.021896</td>\n","      <td>6.053211</td>\n","      <td>5.619469</td>\n","      <td>5.054428</td>\n","      <td>4.255414</td>\n","      <td>5.509953</td>\n","      <td>5.676258</td>\n","      <td>4.904795</td>\n","      <td>3.791634</td>\n","      <td>4.963086</td>\n","      <td>5.596190</td>\n","      <td>4.390768</td>\n","      <td>4.559179</td>\n","      <td>4.246613</td>\n","      <td>5.008229</td>\n","      <td>4.412040</td>\n","      <td>5.410323</td>\n","      <td>3.929521</td>\n","      <td>5.305050</td>\n","      <td>5.714700</td>\n","      <td>5.954384</td>\n","      <td>5.444080</td>\n","      <td>3.726498</td>\n","      <td>3.144466</td>\n","      <td>4.297603</td>\n","      <td>4.530393</td>\n","      <td>4.839852</td>\n","      <td>4.631512</td>\n","      <td>4.660395</td>\n","      <td>4.607325</td>\n","      <td>4.835523</td>\n","      <td>5.799837</td>\n","      <td>5.402324</td>\n","      <td>5.847432</td>\n","      <td>5.286635</td>\n","      <td>...</td>\n","      <td>4.466901</td>\n","      <td>4.678448</td>\n","      <td>5.508823</td>\n","      <td>5.202156</td>\n","      <td>5.836791</td>\n","      <td>4.406157</td>\n","      <td>3.642963</td>\n","      <td>5.840336</td>\n","      <td>4.471259</td>\n","      <td>3.545131</td>\n","      <td>5.967366</td>\n","      <td>4.826879</td>\n","      <td>4.693311</td>\n","      <td>5.242001</td>\n","      <td>3.569369</td>\n","      <td>4.463879</td>\n","      <td>4.327261</td>\n","      <td>4.827139</td>\n","      <td>4.693632</td>\n","      <td>4.047480</td>\n","      <td>4.105528</td>\n","      <td>5.650454</td>\n","      <td>5.441878</td>\n","      <td>4.396009</td>\n","      <td>4.549152</td>\n","      <td>5.253070</td>\n","      <td>4.813044</td>\n","      <td>4.797873</td>\n","      <td>3.852566</td>\n","      <td>4.497807</td>\n","      <td>4.372185</td>\n","      <td>4.341782</td>\n","      <td>4.535543</td>\n","      <td>2.966107</td>\n","      <td>4.379730</td>\n","      <td>4.661430</td>\n","      <td>4.479786</td>\n","      <td>4.537601</td>\n","      <td>5.016662</td>\n","      <td>5.876392</td>\n","    </tr>\n","    <tr>\n","      <th>17718</th>\n","      <td>psiTPTE22|387590</td>\n","      <td>-0.242836</td>\n","      <td>-0.127967</td>\n","      <td>-1.373839</td>\n","      <td>-1.172729</td>\n","      <td>-1.467482</td>\n","      <td>0.446395</td>\n","      <td>-2.301554</td>\n","      <td>-1.195793</td>\n","      <td>-2.740848</td>\n","      <td>0.006735</td>\n","      <td>4.984601</td>\n","      <td>0.606398</td>\n","      <td>-0.639032</td>\n","      <td>-0.543682</td>\n","      <td>1.212051</td>\n","      <td>2.954537</td>\n","      <td>0.142837</td>\n","      <td>-0.364709</td>\n","      <td>0.729040</td>\n","      <td>-2.204410</td>\n","      <td>-2.339090</td>\n","      <td>-0.776198</td>\n","      <td>3.377897</td>\n","      <td>-0.903320</td>\n","      <td>3.872832</td>\n","      <td>-2.069476</td>\n","      <td>-2.404709</td>\n","      <td>-4.496206</td>\n","      <td>-0.496478</td>\n","      <td>1.685091</td>\n","      <td>-1.623047</td>\n","      <td>1.262987</td>\n","      <td>3.011101</td>\n","      <td>-5.540460</td>\n","      <td>-3.609583</td>\n","      <td>0.247669</td>\n","      <td>3.615363</td>\n","      <td>-2.428565</td>\n","      <td>-1.562647</td>\n","      <td>...</td>\n","      <td>0.715167</td>\n","      <td>0.222720</td>\n","      <td>-2.556045</td>\n","      <td>2.171344</td>\n","      <td>0.391473</td>\n","      <td>-0.178142</td>\n","      <td>-2.649918</td>\n","      <td>-0.569548</td>\n","      <td>-3.838842</td>\n","      <td>-0.779562</td>\n","      <td>-1.307834</td>\n","      <td>-0.665448</td>\n","      <td>3.760537</td>\n","      <td>-2.286324</td>\n","      <td>7.403236</td>\n","      <td>2.304732</td>\n","      <td>-1.299710</td>\n","      <td>0.080441</td>\n","      <td>3.845418</td>\n","      <td>-1.066671</td>\n","      <td>-0.090068</td>\n","      <td>-2.574553</td>\n","      <td>3.306915</td>\n","      <td>-3.233426</td>\n","      <td>-0.508828</td>\n","      <td>-0.832805</td>\n","      <td>-2.373957</td>\n","      <td>1.012985</td>\n","      <td>0.199597</td>\n","      <td>3.150084</td>\n","      <td>-1.141041</td>\n","      <td>-1.545836</td>\n","      <td>-1.820596</td>\n","      <td>-1.313832</td>\n","      <td>-1.653056</td>\n","      <td>-0.666252</td>\n","      <td>-2.106968</td>\n","      <td>4.258512</td>\n","      <td>3.645529</td>\n","      <td>1.866726</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>17719 rows × 567 columns</p>\n","</div>"],"text/plain":["                   Gene  TCGA-4P-AA8J  ...  TCGA-WA-A7GZ.1  TCGA-WA-A7H4\n","0           ?|100133144     -2.668367  ...        1.055458      0.110682\n","1           ?|100134869     -0.794596  ...        0.730860      0.065397\n","2               ?|10357      2.411792  ...        2.989047      2.724889\n","3               ?|10431      5.945483  ...        4.833012      5.228947\n","4              ?|155060      3.523286  ...        3.219246      3.645937\n","...                 ...           ...  ...             ...           ...\n","17714      ZYG11B|79699      4.176802  ...        4.644218      5.545347\n","17715          ZYX|7791      8.967494  ...        7.323343      8.053697\n","17716       ZZEF1|23140      5.248309  ...        6.381286      6.527878\n","17717        ZZZ3|26009      3.952576  ...        5.016662      5.876392\n","17718  psiTPTE22|387590     -0.242836  ...        3.645529      1.866726\n","\n","[17719 rows x 567 columns]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"XZVE85dX2VQp","executionInfo":{"status":"ok","timestamp":1621025816588,"user_tz":240,"elapsed":1447,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}}},"source":["# hugo = lambda x : x.split(\"|\")[0]\n","# genes = genes_df.gene.str.replace(r'[a-z]+|[a-z]+', hugo)\n","\n","rna_overlap = rna_df[rna_df['Gene'].apply(lambda g: any([str(x) ==  g.split(\"|\")[0] for x in genes_df.gene.values]))]\n","meth_overlap = meth_df[meth_df.Hugo_Symbol.isin(genes_df.gene)]"],"execution_count":109,"outputs":[]},{"cell_type":"code","metadata":{"id":"yX0aIaAo6aWI","colab":{"base_uri":"https://localhost:8080/","height":779},"executionInfo":{"status":"ok","timestamp":1621025822473,"user_tz":240,"elapsed":559,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"67bdc02b-2e5e-454b-d58b-e77a60a1e269"},"source":["pats_rna = rna_overlap[patients_df.PATIENT_ID]\n","pats_meth = meth_overlap[[pat + \"-01\" for pat in patients_df.PATIENT_ID]]\n","new_names = {x: y for x, y in zip(pats_meth.columns, pats_rna.columns)}\n","X = pd.concat([pats_rna, pats_meth.rename(columns=new_names)], ignore_index=True)\n","X = X.T\n","y = patients_df['BINARY_SURVIVAL']"],"execution_count":111,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","      <th>26</th>\n","      <th>27</th>\n","      <th>28</th>\n","      <th>29</th>\n","      <th>30</th>\n","      <th>31</th>\n","      <th>32</th>\n","      <th>33</th>\n","      <th>34</th>\n","      <th>35</th>\n","      <th>36</th>\n","      <th>37</th>\n","      <th>38</th>\n","      <th>39</th>\n","      <th>...</th>\n","      <th>180</th>\n","      <th>181</th>\n","      <th>182</th>\n","      <th>183</th>\n","      <th>184</th>\n","      <th>185</th>\n","      <th>186</th>\n","      <th>187</th>\n","      <th>188</th>\n","      <th>189</th>\n","      <th>190</th>\n","      <th>191</th>\n","      <th>192</th>\n","      <th>193</th>\n","      <th>194</th>\n","      <th>195</th>\n","      <th>196</th>\n","      <th>197</th>\n","      <th>198</th>\n","      <th>199</th>\n","      <th>200</th>\n","      <th>201</th>\n","      <th>202</th>\n","      <th>203</th>\n","      <th>204</th>\n","      <th>205</th>\n","      <th>206</th>\n","      <th>207</th>\n","      <th>208</th>\n","      <th>209</th>\n","      <th>210</th>\n","      <th>211</th>\n","      <th>212</th>\n","      <th>213</th>\n","      <th>214</th>\n","      <th>215</th>\n","      <th>216</th>\n","      <th>217</th>\n","      <th>218</th>\n","      <th>219</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>TCGA-4P-AA8J</th>\n","      <td>1.557654</td>\n","      <td>2.147765</td>\n","      <td>-2.268722</td>\n","      <td>3.113584</td>\n","      <td>2.671448</td>\n","      <td>0.542092</td>\n","      <td>5.766262</td>\n","      <td>1.321528</td>\n","      <td>-0.288921</td>\n","      <td>2.867989</td>\n","      <td>-3.743013</td>\n","      <td>-1.162944</td>\n","      <td>-0.385739</td>\n","      <td>4.346536</td>\n","      <td>-0.336518</td>\n","      <td>2.464821</td>\n","      <td>-5.535868</td>\n","      <td>-1.939864</td>\n","      <td>-0.787300</td>\n","      <td>2.584665</td>\n","      <td>-1.939864</td>\n","      <td>-1.554947</td>\n","      <td>1.012886</td>\n","      <td>-0.198186</td>\n","      <td>-3.304006</td>\n","      <td>6.315385</td>\n","      <td>0.404096</td>\n","      <td>-4.377273</td>\n","      <td>3.712660</td>\n","      <td>2.873229</td>\n","      <td>-2.268722</td>\n","      <td>3.825877</td>\n","      <td>2.485500</td>\n","      <td>3.468942</td>\n","      <td>-1.079588</td>\n","      <td>-2.470606</td>\n","      <td>-1.000774</td>\n","      <td>2.002293</td>\n","      <td>2.584665</td>\n","      <td>-0.436708</td>\n","      <td>...</td>\n","      <td>0.824512</td>\n","      <td>0.078068</td>\n","      <td>0.356636</td>\n","      <td>0.686795</td>\n","      <td>0.772182</td>\n","      <td>0.842397</td>\n","      <td>0.109853</td>\n","      <td>0.899376</td>\n","      <td>0.885238</td>\n","      <td>0.749016</td>\n","      <td>0.577863</td>\n","      <td>0.441398</td>\n","      <td>0.045949</td>\n","      <td>0.697315</td>\n","      <td>0.805008</td>\n","      <td>0.273417</td>\n","      <td>0.810487</td>\n","      <td>0.672974</td>\n","      <td>0.625042</td>\n","      <td>0.573931</td>\n","      <td>0.022781</td>\n","      <td>0.616018</td>\n","      <td>0.730767</td>\n","      <td>0.901373</td>\n","      <td>0.119325</td>\n","      <td>0.042243</td>\n","      <td>0.479445</td>\n","      <td>0.021783</td>\n","      <td>0.823219</td>\n","      <td>0.034057</td>\n","      <td>0.632986</td>\n","      <td>0.184937</td>\n","      <td>0.715685</td>\n","      <td>0.690234</td>\n","      <td>0.100887</td>\n","      <td>0.720547</td>\n","      <td>0.457601</td>\n","      <td>0.391490</td>\n","      <td>0.813552</td>\n","      <td>0.185931</td>\n","    </tr>\n","    <tr>\n","      <th>TCGA-BA-4074</th>\n","      <td>0.117535</td>\n","      <td>1.213815</td>\n","      <td>-4.817821</td>\n","      <td>-1.195265</td>\n","      <td>-1.747521</td>\n","      <td>-1.824292</td>\n","      <td>3.170226</td>\n","      <td>5.251076</td>\n","      <td>-1.991297</td>\n","      <td>-5.642222</td>\n","      <td>-4.296485</td>\n","      <td>2.742117</td>\n","      <td>0.700464</td>\n","      <td>-0.960526</td>\n","      <td>-1.195265</td>\n","      <td>1.561811</td>\n","      <td>-5.642222</td>\n","      <td>0.780414</td>\n","      <td>4.567379</td>\n","      <td>-1.195265</td>\n","      <td>3.362160</td>\n","      <td>2.700833</td>\n","      <td>1.406199</td>\n","      <td>-4.296485</td>\n","      <td>-1.145159</td>\n","      <td>5.101272</td>\n","      <td>3.206857</td>\n","      <td>-0.615369</td>\n","      <td>4.190978</td>\n","      <td>0.362338</td>\n","      <td>-3.150472</td>\n","      <td>9.093775</td>\n","      <td>2.367126</td>\n","      <td>1.771981</td>\n","      <td>-5.642222</td>\n","      <td>-1.923850</td>\n","      <td>-5.642222</td>\n","      <td>2.731328</td>\n","      <td>4.419159</td>\n","      <td>0.462685</td>\n","      <td>...</td>\n","      <td>0.343530</td>\n","      <td>0.625043</td>\n","      <td>0.754377</td>\n","      <td>0.461467</td>\n","      <td>0.575118</td>\n","      <td>0.297663</td>\n","      <td>0.208854</td>\n","      <td>0.564968</td>\n","      <td>0.465423</td>\n","      <td>0.435238</td>\n","      <td>0.253425</td>\n","      <td>0.563785</td>\n","      <td>0.437677</td>\n","      <td>0.543199</td>\n","      <td>0.721728</td>\n","      <td>0.557288</td>\n","      <td>0.547947</td>\n","      <td>0.362120</td>\n","      <td>0.220177</td>\n","      <td>0.627820</td>\n","      <td>0.023367</td>\n","      <td>0.232283</td>\n","      <td>0.391456</td>\n","      <td>0.395695</td>\n","      <td>0.563010</td>\n","      <td>0.032956</td>\n","      <td>0.319221</td>\n","      <td>0.031235</td>\n","      <td>0.863994</td>\n","      <td>0.032398</td>\n","      <td>0.365447</td>\n","      <td>0.528380</td>\n","      <td>0.322625</td>\n","      <td>0.879799</td>\n","      <td>0.517337</td>\n","      <td>0.411522</td>\n","      <td>0.382505</td>\n","      <td>0.331764</td>\n","      <td>0.765983</td>\n","      <td>0.203692</td>\n","    </tr>\n","    <tr>\n","      <th>TCGA-BA-4076</th>\n","      <td>-3.434160</td>\n","      <td>0.488682</td>\n","      <td>-5.486715</td>\n","      <td>-3.108814</td>\n","      <td>3.913738</td>\n","      <td>-0.561437</td>\n","      <td>-2.843505</td>\n","      <td>4.284498</td>\n","      <td>0.604249</td>\n","      <td>1.211595</td>\n","      <td>-5.486715</td>\n","      <td>4.035935</td>\n","      <td>-3.854912</td>\n","      <td>5.829639</td>\n","      <td>-4.451513</td>\n","      <td>-1.027992</td>\n","      <td>-1.614575</td>\n","      <td>-0.465075</td>\n","      <td>6.228113</td>\n","      <td>3.579973</td>\n","      <td>-0.775936</td>\n","      <td>-0.960734</td>\n","      <td>-2.836226</td>\n","      <td>-2.425595</td>\n","      <td>-5.486715</td>\n","      <td>5.605955</td>\n","      <td>-2.254729</td>\n","      <td>-3.854912</td>\n","      <td>5.240703</td>\n","      <td>3.078593</td>\n","      <td>-3.854912</td>\n","      <td>6.130280</td>\n","      <td>2.706804</td>\n","      <td>4.334391</td>\n","      <td>0.690466</td>\n","      <td>-4.429438</td>\n","      <td>-0.331613</td>\n","      <td>-0.374740</td>\n","      <td>4.066647</td>\n","      <td>-1.250926</td>\n","      <td>...</td>\n","      <td>0.974058</td>\n","      <td>0.144655</td>\n","      <td>0.760847</td>\n","      <td>0.953548</td>\n","      <td>0.935956</td>\n","      <td>0.935991</td>\n","      <td>0.061421</td>\n","      <td>0.940074</td>\n","      <td>0.934582</td>\n","      <td>0.768129</td>\n","      <td>0.611844</td>\n","      <td>0.835582</td>\n","      <td>0.025003</td>\n","      <td>0.935789</td>\n","      <td>0.888934</td>\n","      <td>0.150080</td>\n","      <td>0.589548</td>\n","      <td>0.871340</td>\n","      <td>0.767691</td>\n","      <td>0.163818</td>\n","      <td>0.019472</td>\n","      <td>0.443519</td>\n","      <td>0.431055</td>\n","      <td>0.914077</td>\n","      <td>0.145124</td>\n","      <td>0.062896</td>\n","      <td>0.897556</td>\n","      <td>0.018016</td>\n","      <td>0.858426</td>\n","      <td>0.020677</td>\n","      <td>0.920255</td>\n","      <td>0.611491</td>\n","      <td>0.720849</td>\n","      <td>0.936966</td>\n","      <td>0.045591</td>\n","      <td>0.270206</td>\n","      <td>0.356397</td>\n","      <td>0.731391</td>\n","      <td>0.759805</td>\n","      <td>0.780626</td>\n","    </tr>\n","    <tr>\n","      <th>TCGA-BA-4078</th>\n","      <td>-0.700590</td>\n","      <td>2.450555</td>\n","      <td>-4.700674</td>\n","      <td>-1.046276</td>\n","      <td>-0.300482</td>\n","      <td>-3.950647</td>\n","      <td>-1.936023</td>\n","      <td>1.228382</td>\n","      <td>2.613955</td>\n","      <td>1.059160</td>\n","      <td>2.598423</td>\n","      <td>4.057931</td>\n","      <td>-2.452676</td>\n","      <td>-3.950647</td>\n","      <td>-1.085882</td>\n","      <td>5.322391</td>\n","      <td>-0.064620</td>\n","      <td>-0.188475</td>\n","      <td>5.985567</td>\n","      <td>-0.447491</td>\n","      <td>3.764990</td>\n","      <td>1.453779</td>\n","      <td>-0.157340</td>\n","      <td>0.221531</td>\n","      <td>1.796368</td>\n","      <td>1.368443</td>\n","      <td>1.604447</td>\n","      <td>-2.259176</td>\n","      <td>5.018453</td>\n","      <td>-0.188475</td>\n","      <td>4.043723</td>\n","      <td>-1.556392</td>\n","      <td>0.664949</td>\n","      <td>-0.167077</td>\n","      <td>2.404855</td>\n","      <td>-1.729361</td>\n","      <td>-1.672196</td>\n","      <td>4.313825</td>\n","      <td>-2.010289</td>\n","      <td>-2.802493</td>\n","      <td>...</td>\n","      <td>0.861729</td>\n","      <td>0.082795</td>\n","      <td>0.899534</td>\n","      <td>0.386581</td>\n","      <td>0.922714</td>\n","      <td>0.943590</td>\n","      <td>0.034701</td>\n","      <td>0.407754</td>\n","      <td>0.316825</td>\n","      <td>0.893290</td>\n","      <td>0.889760</td>\n","      <td>0.626806</td>\n","      <td>0.027019</td>\n","      <td>0.552585</td>\n","      <td>0.143877</td>\n","      <td>0.191715</td>\n","      <td>0.929874</td>\n","      <td>0.266408</td>\n","      <td>0.710635</td>\n","      <td>0.889777</td>\n","      <td>0.017942</td>\n","      <td>0.630519</td>\n","      <td>0.652138</td>\n","      <td>0.938323</td>\n","      <td>0.178727</td>\n","      <td>0.057087</td>\n","      <td>0.864553</td>\n","      <td>0.015648</td>\n","      <td>0.210194</td>\n","      <td>0.016601</td>\n","      <td>0.913071</td>\n","      <td>0.659434</td>\n","      <td>0.446729</td>\n","      <td>0.833059</td>\n","      <td>0.095818</td>\n","      <td>0.888664</td>\n","      <td>0.837036</td>\n","      <td>0.862937</td>\n","      <td>0.412536</td>\n","      <td>0.126813</td>\n","    </tr>\n","    <tr>\n","      <th>TCGA-BA-5149</th>\n","      <td>-4.261041</td>\n","      <td>-3.894903</td>\n","      <td>-2.533619</td>\n","      <td>-2.971457</td>\n","      <td>6.199639</td>\n","      <td>-2.665319</td>\n","      <td>3.118804</td>\n","      <td>4.845919</td>\n","      <td>-3.152900</td>\n","      <td>-2.665319</td>\n","      <td>-5.505319</td>\n","      <td>-2.810261</td>\n","      <td>-3.152900</td>\n","      <td>4.992434</td>\n","      <td>-4.753085</td>\n","      <td>-4.261041</td>\n","      <td>-2.665319</td>\n","      <td>-2.301554</td>\n","      <td>3.444500</td>\n","      <td>-0.786947</td>\n","      <td>-1.926212</td>\n","      <td>-5.505319</td>\n","      <td>-3.114762</td>\n","      <td>-5.505319</td>\n","      <td>-5.505319</td>\n","      <td>5.614153</td>\n","      <td>-1.500081</td>\n","      <td>-3.603130</td>\n","      <td>6.413931</td>\n","      <td>-0.987867</td>\n","      <td>-4.753085</td>\n","      <td>6.747706</td>\n","      <td>-2.011288</td>\n","      <td>0.524562</td>\n","      <td>-2.533619</td>\n","      <td>-5.505319</td>\n","      <td>-2.101724</td>\n","      <td>-0.087393</td>\n","      <td>-2.412943</td>\n","      <td>1.419082</td>\n","      <td>...</td>\n","      <td>0.718630</td>\n","      <td>0.446445</td>\n","      <td>0.630731</td>\n","      <td>0.845140</td>\n","      <td>0.309942</td>\n","      <td>0.801773</td>\n","      <td>0.279054</td>\n","      <td>0.884827</td>\n","      <td>0.875000</td>\n","      <td>0.531096</td>\n","      <td>0.245380</td>\n","      <td>0.631518</td>\n","      <td>0.050744</td>\n","      <td>0.863449</td>\n","      <td>0.808881</td>\n","      <td>0.436152</td>\n","      <td>0.558456</td>\n","      <td>0.452838</td>\n","      <td>0.741747</td>\n","      <td>0.762071</td>\n","      <td>0.035764</td>\n","      <td>0.275109</td>\n","      <td>0.561434</td>\n","      <td>0.919388</td>\n","      <td>0.130744</td>\n","      <td>0.438465</td>\n","      <td>0.489331</td>\n","      <td>0.204130</td>\n","      <td>0.719062</td>\n","      <td>0.033603</td>\n","      <td>0.690240</td>\n","      <td>0.644358</td>\n","      <td>0.696753</td>\n","      <td>0.711030</td>\n","      <td>0.087528</td>\n","      <td>0.400615</td>\n","      <td>0.487990</td>\n","      <td>0.253125</td>\n","      <td>0.844178</td>\n","      <td>0.262596</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>TCGA-UF-A7JT</th>\n","      <td>-2.914661</td>\n","      <td>-4.645924</td>\n","      <td>-4.095972</td>\n","      <td>-1.571318</td>\n","      <td>-1.739704</td>\n","      <td>1.569714</td>\n","      <td>0.005410</td>\n","      <td>5.241212</td>\n","      <td>-0.105291</td>\n","      <td>-5.545719</td>\n","      <td>-5.545719</td>\n","      <td>-2.273994</td>\n","      <td>-1.930373</td>\n","      <td>-1.653056</td>\n","      <td>-5.545719</td>\n","      <td>-4.095972</td>\n","      <td>2.582471</td>\n","      <td>-2.914661</td>\n","      <td>3.453781</td>\n","      <td>1.016622</td>\n","      <td>-0.256799</td>\n","      <td>1.533224</td>\n","      <td>-0.985163</td>\n","      <td>-3.698764</td>\n","      <td>-2.409494</td>\n","      <td>3.312895</td>\n","      <td>-2.914661</td>\n","      <td>-2.914661</td>\n","      <td>6.002050</td>\n","      <td>1.437745</td>\n","      <td>-5.545719</td>\n","      <td>6.081711</td>\n","      <td>-0.887941</td>\n","      <td>6.261858</td>\n","      <td>-4.095972</td>\n","      <td>-5.545719</td>\n","      <td>2.407041</td>\n","      <td>-0.702458</td>\n","      <td>1.715257</td>\n","      <td>6.286046</td>\n","      <td>...</td>\n","      <td>0.962170</td>\n","      <td>0.485960</td>\n","      <td>0.378191</td>\n","      <td>0.779746</td>\n","      <td>0.709982</td>\n","      <td>0.921330</td>\n","      <td>0.445444</td>\n","      <td>0.924899</td>\n","      <td>0.903828</td>\n","      <td>0.774741</td>\n","      <td>0.679809</td>\n","      <td>0.320866</td>\n","      <td>0.188757</td>\n","      <td>0.749119</td>\n","      <td>0.695435</td>\n","      <td>0.265996</td>\n","      <td>0.571618</td>\n","      <td>0.512293</td>\n","      <td>0.623209</td>\n","      <td>0.660211</td>\n","      <td>0.064686</td>\n","      <td>0.613846</td>\n","      <td>0.462801</td>\n","      <td>0.904518</td>\n","      <td>0.139187</td>\n","      <td>0.421844</td>\n","      <td>0.672461</td>\n","      <td>0.317561</td>\n","      <td>0.622364</td>\n","      <td>0.025240</td>\n","      <td>0.854630</td>\n","      <td>0.545439</td>\n","      <td>0.768487</td>\n","      <td>0.776014</td>\n","      <td>0.398115</td>\n","      <td>0.395131</td>\n","      <td>0.557801</td>\n","      <td>0.419677</td>\n","      <td>0.878153</td>\n","      <td>0.561419</td>\n","    </tr>\n","    <tr>\n","      <th>TCGA-UF-A7JV</th>\n","      <td>-1.619289</td>\n","      <td>-1.692166</td>\n","      <td>-4.240107</td>\n","      <td>-0.493479</td>\n","      <td>0.328219</td>\n","      <td>-3.307302</td>\n","      <td>-1.089882</td>\n","      <td>-1.493470</td>\n","      <td>-2.598116</td>\n","      <td>-1.301641</td>\n","      <td>-1.420445</td>\n","      <td>-1.768920</td>\n","      <td>-2.909564</td>\n","      <td>4.879074</td>\n","      <td>-4.760865</td>\n","      <td>1.029368</td>\n","      <td>-5.583799</td>\n","      <td>0.656397</td>\n","      <td>-1.935885</td>\n","      <td>1.029368</td>\n","      <td>2.162947</td>\n","      <td>-0.339201</td>\n","      <td>2.623740</td>\n","      <td>-5.583799</td>\n","      <td>-4.760865</td>\n","      <td>3.455760</td>\n","      <td>1.774016</td>\n","      <td>-5.583799</td>\n","      <td>2.370117</td>\n","      <td>-3.556645</td>\n","      <td>-4.240107</td>\n","      <td>4.157901</td>\n","      <td>-4.760865</td>\n","      <td>-0.339201</td>\n","      <td>-2.342111</td>\n","      <td>-4.705505</td>\n","      <td>-4.240107</td>\n","      <td>3.330286</td>\n","      <td>-1.692166</td>\n","      <td>1.555339</td>\n","      <td>...</td>\n","      <td>0.966675</td>\n","      <td>0.155880</td>\n","      <td>0.502347</td>\n","      <td>0.818883</td>\n","      <td>0.780719</td>\n","      <td>0.946178</td>\n","      <td>0.477068</td>\n","      <td>0.845866</td>\n","      <td>0.874850</td>\n","      <td>0.701119</td>\n","      <td>0.234684</td>\n","      <td>0.401624</td>\n","      <td>0.362328</td>\n","      <td>0.900951</td>\n","      <td>0.874649</td>\n","      <td>0.565891</td>\n","      <td>0.292130</td>\n","      <td>0.565000</td>\n","      <td>0.514473</td>\n","      <td>0.318001</td>\n","      <td>0.026694</td>\n","      <td>0.507489</td>\n","      <td>0.835424</td>\n","      <td>0.934793</td>\n","      <td>0.165251</td>\n","      <td>0.093786</td>\n","      <td>0.880604</td>\n","      <td>0.018230</td>\n","      <td>0.521267</td>\n","      <td>0.065221</td>\n","      <td>0.610832</td>\n","      <td>0.230041</td>\n","      <td>0.703787</td>\n","      <td>0.829275</td>\n","      <td>0.489235</td>\n","      <td>0.830097</td>\n","      <td>0.159317</td>\n","      <td>0.444403</td>\n","      <td>0.895318</td>\n","      <td>0.206352</td>\n","    </tr>\n","    <tr>\n","      <th>TCGA-UP-A6WW</th>\n","      <td>-1.470776</td>\n","      <td>-5.437870</td>\n","      <td>-4.178327</td>\n","      <td>0.587464</td>\n","      <td>-1.234074</td>\n","      <td>2.704907</td>\n","      <td>-4.674714</td>\n","      <td>0.908544</td>\n","      <td>1.980777</td>\n","      <td>4.270923</td>\n","      <td>0.748036</td>\n","      <td>3.391249</td>\n","      <td>-1.289701</td>\n","      <td>-2.322138</td>\n","      <td>-0.895154</td>\n","      <td>0.508121</td>\n","      <td>-5.437870</td>\n","      <td>5.585307</td>\n","      <td>3.187347</td>\n","      <td>-4.178327</td>\n","      <td>3.371231</td>\n","      <td>-2.882103</td>\n","      <td>1.340556</td>\n","      <td>0.159160</td>\n","      <td>0.028776</td>\n","      <td>-4.178327</td>\n","      <td>1.293628</td>\n","      <td>4.537871</td>\n","      <td>0.824837</td>\n","      <td>1.702894</td>\n","      <td>-4.178327</td>\n","      <td>0.316488</td>\n","      <td>-3.516282</td>\n","      <td>-2.443110</td>\n","      <td>-1.754084</td>\n","      <td>-4.061358</td>\n","      <td>-3.272569</td>\n","      <td>3.318717</td>\n","      <td>4.116304</td>\n","      <td>-3.064194</td>\n","      <td>...</td>\n","      <td>0.947387</td>\n","      <td>0.462004</td>\n","      <td>0.838649</td>\n","      <td>0.279637</td>\n","      <td>0.866016</td>\n","      <td>0.930555</td>\n","      <td>0.173666</td>\n","      <td>0.697273</td>\n","      <td>0.893800</td>\n","      <td>0.888353</td>\n","      <td>0.918724</td>\n","      <td>0.520251</td>\n","      <td>0.604688</td>\n","      <td>0.841567</td>\n","      <td>0.134103</td>\n","      <td>0.138331</td>\n","      <td>0.916426</td>\n","      <td>0.810712</td>\n","      <td>0.734991</td>\n","      <td>0.902552</td>\n","      <td>0.023177</td>\n","      <td>0.889781</td>\n","      <td>0.833390</td>\n","      <td>0.927743</td>\n","      <td>0.580004</td>\n","      <td>0.696434</td>\n","      <td>0.846038</td>\n","      <td>0.024021</td>\n","      <td>0.203798</td>\n","      <td>0.217808</td>\n","      <td>0.556491</td>\n","      <td>0.808421</td>\n","      <td>0.162766</td>\n","      <td>0.505935</td>\n","      <td>0.683882</td>\n","      <td>0.872153</td>\n","      <td>0.748516</td>\n","      <td>0.891500</td>\n","      <td>0.741111</td>\n","      <td>0.754107</td>\n","    </tr>\n","    <tr>\n","      <th>TCGA-WA-A7GZ</th>\n","      <td>5.253143</td>\n","      <td>3.537264</td>\n","      <td>6.339071</td>\n","      <td>-2.055025</td>\n","      <td>-0.899840</td>\n","      <td>-1.276116</td>\n","      <td>3.605937</td>\n","      <td>1.863178</td>\n","      <td>5.264271</td>\n","      <td>0.820785</td>\n","      <td>-5.705009</td>\n","      <td>-3.067028</td>\n","      <td>2.028413</td>\n","      <td>-1.560310</td>\n","      <td>0.349592</td>\n","      <td>3.916871</td>\n","      <td>1.750999</td>\n","      <td>2.808995</td>\n","      <td>2.625568</td>\n","      <td>6.382271</td>\n","      <td>3.715395</td>\n","      <td>-3.870136</td>\n","      <td>1.181193</td>\n","      <td>-0.266649</td>\n","      <td>-5.705009</td>\n","      <td>-0.835068</td>\n","      <td>0.800546</td>\n","      <td>-5.705009</td>\n","      <td>-0.266649</td>\n","      <td>-3.870136</td>\n","      <td>-0.835068</td>\n","      <td>4.271439</td>\n","      <td>3.611757</td>\n","      <td>3.859327</td>\n","      <td>-0.067503</td>\n","      <td>2.155613</td>\n","      <td>1.251919</td>\n","      <td>3.930906</td>\n","      <td>5.071860</td>\n","      <td>-1.113652</td>\n","      <td>...</td>\n","      <td>0.939459</td>\n","      <td>0.284465</td>\n","      <td>0.210265</td>\n","      <td>0.405539</td>\n","      <td>0.838621</td>\n","      <td>0.411853</td>\n","      <td>0.263998</td>\n","      <td>0.753673</td>\n","      <td>0.509607</td>\n","      <td>0.741581</td>\n","      <td>0.850124</td>\n","      <td>0.507621</td>\n","      <td>0.278493</td>\n","      <td>0.674053</td>\n","      <td>0.606559</td>\n","      <td>0.176957</td>\n","      <td>0.465671</td>\n","      <td>0.319173</td>\n","      <td>0.551235</td>\n","      <td>0.727509</td>\n","      <td>0.025908</td>\n","      <td>0.785253</td>\n","      <td>0.850223</td>\n","      <td>0.924216</td>\n","      <td>0.582392</td>\n","      <td>0.555219</td>\n","      <td>0.457637</td>\n","      <td>0.285966</td>\n","      <td>0.523924</td>\n","      <td>0.027435</td>\n","      <td>0.605845</td>\n","      <td>0.608865</td>\n","      <td>0.717535</td>\n","      <td>0.710800</td>\n","      <td>0.038365</td>\n","      <td>0.843027</td>\n","      <td>0.236145</td>\n","      <td>0.263464</td>\n","      <td>0.515262</td>\n","      <td>0.220596</td>\n","    </tr>\n","    <tr>\n","      <th>TCGA-WA-A7H4</th>\n","      <td>1.850538</td>\n","      <td>1.817608</td>\n","      <td>-0.252839</td>\n","      <td>0.860660</td>\n","      <td>3.971964</td>\n","      <td>-5.299470</td>\n","      <td>5.592343</td>\n","      <td>1.254333</td>\n","      <td>1.502426</td>\n","      <td>-0.653399</td>\n","      <td>1.013878</td>\n","      <td>-2.127494</td>\n","      <td>2.004779</td>\n","      <td>2.442948</td>\n","      <td>3.356038</td>\n","      <td>5.741347</td>\n","      <td>-3.227295</td>\n","      <td>-0.960988</td>\n","      <td>1.013878</td>\n","      <td>4.843668</td>\n","      <td>3.230312</td>\n","      <td>-2.127494</td>\n","      <td>7.913430</td>\n","      <td>-1.688533</td>\n","      <td>-3.919516</td>\n","      <td>1.990077</td>\n","      <td>1.783908</td>\n","      <td>-1.688533</td>\n","      <td>6.527757</td>\n","      <td>2.820735</td>\n","      <td>0.860660</td>\n","      <td>5.513572</td>\n","      <td>2.804020</td>\n","      <td>2.103706</td>\n","      <td>-1.891383</td>\n","      <td>-2.264620</td>\n","      <td>-2.409957</td>\n","      <td>3.687539</td>\n","      <td>5.427524</td>\n","      <td>1.070883</td>\n","      <td>...</td>\n","      <td>0.791490</td>\n","      <td>0.196973</td>\n","      <td>0.340278</td>\n","      <td>0.663522</td>\n","      <td>0.759922</td>\n","      <td>0.732739</td>\n","      <td>0.088513</td>\n","      <td>0.822737</td>\n","      <td>0.595294</td>\n","      <td>0.644214</td>\n","      <td>0.654187</td>\n","      <td>0.194132</td>\n","      <td>0.208901</td>\n","      <td>0.697316</td>\n","      <td>0.723742</td>\n","      <td>0.473356</td>\n","      <td>0.678429</td>\n","      <td>0.578003</td>\n","      <td>0.582487</td>\n","      <td>0.466695</td>\n","      <td>0.023324</td>\n","      <td>0.766415</td>\n","      <td>0.651293</td>\n","      <td>0.808527</td>\n","      <td>0.129393</td>\n","      <td>0.042754</td>\n","      <td>0.614544</td>\n","      <td>0.015853</td>\n","      <td>0.518274</td>\n","      <td>0.028417</td>\n","      <td>0.584594</td>\n","      <td>0.147502</td>\n","      <td>0.493018</td>\n","      <td>0.674625</td>\n","      <td>0.105538</td>\n","      <td>0.600944</td>\n","      <td>0.677201</td>\n","      <td>0.572131</td>\n","      <td>0.753053</td>\n","      <td>0.143080</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>438 rows × 220 columns</p>\n","</div>"],"text/plain":["                   0         1         2    ...       217       218       219\n","TCGA-4P-AA8J  1.557654  2.147765 -2.268722  ...  0.391490  0.813552  0.185931\n","TCGA-BA-4074  0.117535  1.213815 -4.817821  ...  0.331764  0.765983  0.203692\n","TCGA-BA-4076 -3.434160  0.488682 -5.486715  ...  0.731391  0.759805  0.780626\n","TCGA-BA-4078 -0.700590  2.450555 -4.700674  ...  0.862937  0.412536  0.126813\n","TCGA-BA-5149 -4.261041 -3.894903 -2.533619  ...  0.253125  0.844178  0.262596\n","...                ...       ...       ...  ...       ...       ...       ...\n","TCGA-UF-A7JT -2.914661 -4.645924 -4.095972  ...  0.419677  0.878153  0.561419\n","TCGA-UF-A7JV -1.619289 -1.692166 -4.240107  ...  0.444403  0.895318  0.206352\n","TCGA-UP-A6WW -1.470776 -5.437870 -4.178327  ...  0.891500  0.741111  0.754107\n","TCGA-WA-A7GZ  5.253143  3.537264  6.339071  ...  0.263464  0.515262  0.220596\n","TCGA-WA-A7H4  1.850538  1.817608 -0.252839  ...  0.572131  0.753053  0.143080\n","\n","[438 rows x 220 columns]"]},"metadata":{"tags":[]},"execution_count":111}]},{"cell_type":"code","metadata":{"id":"bK4am3fb9X_1"},"source":["pats_df = patients_df[~patients_df.PATIENT_ID.isin(['TCGA-CQ-A4CA','TCGA-H7-A6C4'])]\n","pats_rna_lin = rna_overlap[pats_df.PATIENT_ID]\n","pats_meth_lin = meth_overlap[[pat + \"-01\" for pat in pats_df.PATIENT_ID]]\n","new_names_lin = {x: y for x, y in zip(pats_meth.columns, pats_rna.columns)}\n","X_lreg = pd.concat([pats_rna_lin, pats_meth_lin.rename(columns=new_names_lin)], ignore_index=True)\n","X_lreg = X_lreg.T\n","y_lreg = pats_df['OS_MONTHS']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1k7rV98QoA5s"},"source":["## Models"]},{"cell_type":"code","metadata":{"id":"4Moz3-WsnQKh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620494195583,"user_tz":240,"elapsed":6261,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"8abd93d7-eca8-479e-8cac-dff38f87d3e2"},"source":["clf = RandomForestClassifier()\n","cv_results = cross_validate(clf, X, y, cv=5, scoring='roc_auc')\n","print(\"RFC\", np.mean(cv_results['test_score']))\n","\n","#try l1 regularization \n","clf = LogisticRegression(penalty='l1', solver='liblinear', C=1, class_weight='balanced')\n","cv_results = cross_validate(clf, X, y, cv=5, scoring='roc_auc')\n","print(\"LR\", np.mean(cv_results['test_score']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["RFC 0.5870563973063974\n","LR 0.6134461279461279\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0yBPbvVJ-3RM"},"source":["RFC: 0.59 \\\\\n","LR: 0.61 \n","\n","Comp: \\\\\n","RFC : Meth 56 | RNA 62 | Overlap 59 \\\\\n","LR: Meth 55 | RNA 59 | Overlap 61"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X5aolWRAEMme","executionInfo":{"status":"ok","timestamp":1620495645005,"user_tz":240,"elapsed":2773,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"9ed402f4-2d60-4255-9b3b-5c7d5ef0b3ad"},"source":["clf = RandomForestClassifier()\n","cv_results = cross_validate(clf, pats_rna.T, y, cv=5, scoring='roc_auc')\n","print(\"RFC\", np.mean(cv_results['test_score']))\n","\n","#try l1 regularization \n","clf = LogisticRegression(penalty='l1', solver='liblinear', C=1, class_weight='balanced')\n","cv_results = cross_validate(clf, pats_rna.T, y, cv=5, scoring='roc_auc')\n","print(\"LR\", np.mean(cv_results['test_score']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["RFC 0.5937037037037036\n","LR 0.6138265993265992\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4AlXn6e0E9-Y"},"source":["RNA OG: 59 | 61 or 61 | 61 \\\\\n","RNA New: 59 | 61\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xiCcmfpREYIT","executionInfo":{"status":"ok","timestamp":1620496261020,"user_tz":240,"elapsed":2681,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"b72935b0-a303-41f8-d758-138ead430436"},"source":["clf = RandomForestClassifier()\n","cv_results = cross_validate(clf, pats_meth.T, y, cv=5, scoring='roc_auc')\n","print(\"RFC\", np.mean(cv_results['test_score']))\n","\n","#try l1 regularization \n","clf = LogisticRegression(penalty='l1', solver='liblinear', C=1, class_weight='balanced')\n","cv_results = cross_validate(clf, pats_meth.T, y, cv=5, scoring='roc_auc')\n","print(\"LR\", np.mean(cv_results['test_score']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["RFC 0.5657651515151515\n","LR 0.5776818181818182\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VIatWRDsE_m2"},"source":["Meth OG: 57 | 58 or 59 | 58 \\\\\n","Meth New: 56 | 58"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DlVtVWFX-1kc","executionInfo":{"status":"ok","timestamp":1620494305017,"user_tz":240,"elapsed":721,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"0aaf4b3f-5dbe-491a-ea06-9247030e0854"},"source":["lreg = LinearRegression()\n","cv_results = cross_validate(lreg, X_lreg, y_lreg, cv=5, return_train_score = True)\n","print(\"LinReg\", np.mean(cv_results['test_score']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["LinReg -2.3077217441524756\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oGSJR2Ye_O8F"},"source":["-2.31"]},{"cell_type":"markdown","metadata":{"id":"toyVI7MwepfJ"},"source":["# $R^{2}$ Plot"]},{"cell_type":"markdown","metadata":{"id":"OI3Gd5EFnDcc"},"source":["##Load Counts"]},{"cell_type":"code","metadata":{"id":"2kUNIQXlnDBf","colab":{"base_uri":"https://localhost:8080/","height":231},"executionInfo":{"status":"error","timestamp":1621039948945,"user_tz":240,"elapsed":292,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"8884f718-c2e4-4b43-ffa7-933192d7d9a5"},"source":["#Path to merged mut count\n","mut_count_path = Path(eileen_path + \"merged_mut-count.csv\")\n","merged_df = pd.read_csv(mut_count_path)\n","\n","#Drop bad vals\n","merged_df['mutation_count'].replace('[Not Available]', np.nan, inplace=True)\n","merged_df['OS_MONTHS'].replace('[Not Available]', np.nan, inplace=True)\n","merged_df = merged_df.dropna(subset=['mutation_count', 'OS_MONTHS'])\n","\n","#Get Rows\n","mut_counts = merged_df['mutation_count']\n","survival = merged_df['OS_MONTHS']"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-6ec3731f3d34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Path to merged mut count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmut_count_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meileen_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"merged_mut-count.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmerged_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmut_count_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Drop bad vals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"_TA1hn6OnF0b"},"source":["## Calculate and Plot"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":298},"id":"uR3Cfm41euXF","executionInfo":{"status":"ok","timestamp":1620490628314,"user_tz":240,"elapsed":875,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"40e6d0ea-af33-4ccf-d424-5075c62a33a5"},"source":["X = pd.to_numeric(mut_counts)\n","y = pd.to_numeric(survival)\n","\n","corr_mat = np.corrcoef(X,y)\n","r = corr_mat[0,1]\n","r_sq = r**2\n","\n","m, b = np.polyfit(X, y, 1)\n","\n","plt.plot(X, y, 'o')\n","plt.plot(X, m*X + b)\n","plt.title(\"r^2 = \" + str(r_sq))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 1.0, 'r^2 = 0.00039844884338078814')"]},"metadata":{"tags":[]},"execution_count":33},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3df5xcdX3v8ddnN5NkgjYbII1kAUMthpJSEtxaemMtP7wGRSUFQa1W9HovfTyKD4V6UxevV8FqjU21tNdqpWqLFRFEGlG4oiVYJZVcFxKIEVIpv8JCIAqLSDbJZvdz/zjnbM7OnjNzZnZ+nbPv5+Mxj50558yZ7/zYz3zn8/1l7o6IiBRLT6cLICIizafgLiJSQAruIiIFpOAuIlJACu4iIgWk4C4iUkAK7jljZu8ysz/pdDlEpLspuOeIma0DrgDeZ2Yfrdg3z8y+YGaPmNlzZrbNzF7TonJcama7zewXZvZFM5tX5dgzzex+M9trZreb2YsryvzF8Dy7zexP67jvX5rZrvC+j5jZByru+3oz+7GZ/dLM/t3MToztMzP7qJkNm9mzZvY9M1uRUPbDzWyPmd2R8tw+ZGZuZq+quM91ZvZzM/uZmV1jZr8S27/SzH4QPu5jZva/6zh36nM2syPNbHP4uCNm9kMzW531OVd7L8zsreHrGF32hmV7Wey+f29mT5rZ02b2TTPrj91/mZndYmbPhOf+tJnNie2/ysx2mtmEmb0j6fUIj7stfNw5acdIjLvr0qUXYE7s+oXAg8CvAf3AfcC7Y/sPAy4HlhF8ab8OeA5Y1uQyrQGeBFYAi4DvAetTjj0SeBY4H5gPbADujO3/OPCD8Dy/AewGzsp43+XAYeH1fmAHcG54+3jgF8ArgDnAZcAD0esJXAA8Hr6WvWE57k4o/z8A3wfuSNj3EmB7eJ5XxbZ/BvgO8CvAQuBfgU/F9v8E+Fj4uC8BngDekPHc1Z7z/HB/D2DAWuDprM+52nuR8NzfAfwnYOHtPwPuAZaE5fgScGPs+FuAfwr3vSh8bu+J7b8YOBMYAt6R8phvDd8LJ/Z/oUuV/9VOF0CXijcEHgbeD9wL7A+D09lhUDgmdtyvAtuA86uc617gvCaX7yvAX8RunwnsTjn2IuDfY7cPA0aBE8LbjwOvju3/c+CrWe5b8Tj9YcD4s/D2u4GbY/t7wvueGd5+P3B9bP8KYF/FOf8L8EPgnSQH928Drw3fr3gA/r/An8RuXwzcGru9FzgxdvtrwGVZzl3tOVfs6wFeHwbCX83ynKu9Fwnnvx34cOz2Z4G/jN0+G9gZu30f8NrY7Q3A5xLOewcJwZ3gS/I/gFNRcM98UVqmO72F4B+kz90PuvvN7n6iu++KDnD3p9x9pbt/LekEZrYEeClB7S5p/yvCn+9pl1eklG0FQS0tcg+wxMyOqHWsuz9PUONbYWaLgKMSzrWi1n1jz2HQzH4JPEYQ/L8Sf4oV1w34zfD2V4GXmNlLzaxE8Kvo27Hz9gKfJviSmDY/h5mdD+x391sSnvPfAa8zs0XhczyPIOBHrgTebmYlM1sO/C5B7T7LuWs9Z8zsXmAfcBPweXd/qtZzzvBexM//YuCVBLXzyBeA1Wa21MwWENSyK5/zm81sQZiueQ2x1zuDvyD4Atldx31mPQX37vS37r7L3UcbuXP4z3sNcLW73590jLvf4e59VS6JeWbgBQTpkkh0/YUZjo2Of2G4D6afKzpPtftGz2F9ePsU4J9jx/8r8PtmdpqZzQU+AMwFFoT7nyCoJe4kqNGfD1wae5z3AFvc/a7KJ2RmLyQINu9NeL4Ad4eP9fPwMk6Qqol8C3hj+Lj3A19w9x9lPHe15xzt/y2ClNAfhs8xUu0513ov4t4O/MDdH4pt+ymwCxgmSIf9BvCR2P7vE3xR/ILgS2kI2Jj2HOPMbABYDfyfLMfLIQru3WlX7UOSmVkPwT/9AYKaZ7P9kiB4RKLrz2U4Njr+uXAfTD9XdJ5q953kga0EAeuKcNv9BDXTTxMEtSMJ0lqPhXf7EPDbwDEEeeArgE1hzXIpQXD/XwnPB4J2jX9294dT9l9PkEJ4YVje/wS+DEFjK0GN9SPh4x4DrLFDvZ9qnTv1OVfs3+fu1wKDZnZyredM7fci7u3A1RXb/g6YBxxB8GviRsKae/h5/Ha47TCC92IR8IlqzzF2388A73X3g7WOlwqdzgvpMvVClTxrhvsa8I8EOdFyjWN/j+CfOu3yeyn3+wrwsdjtM6iec98cu30YQc45nnP/r7H9H2Fqzj31vgmP9UHgGyn7+sLnFD3utwgCRvyYEWCAoCFyH0EKYDdBDfZAeL2XoJ3jZ7H94wQNl+8Pz/NL4OTYeVcCvwyvDwDPVDzuJcC3wutVz13Pcw73PwD8Qa3nXOu9iG1bDTwPvLBi+4+BcypebycI5EeG1xfG9q8FfpxQ3ik59/A8E7HXY094rt1pn09dYq9npwugS8UbMrPg/vfAncALWli+s8J/rhPDf75NpPeWWRwGx/MIaoufYGqPl/XAvxHU5E4gqGWfVeu+BL84/zi8nwEvD+8b74HxsjAYLyaoTX8ltu/DYSBZEp7rj8Kg1UdQA31R7PJeYAvwovC+R1Ts30WQ4nhBuP92ghRCObx8hrBhmKA2PEKQMukJ7/9Dwgbqaueu9ZwJGhtfQZASKhM0oD4HLK31nGu9F7HX7SrgSwnv8z8CXydo+CwRpMGGY/sfBAYJOgf0Af9S8X7MDd/jzcD/CK9HvX7ir8dvEwT3fmBup/9Xu/3S8QLoUvGGNBjcgReHH/x9TK2Bv7UFZfxTgu6Qvwj/sefF9u2IPybwKoLc8ihBt8llsX3zgC+G53kS+NOKx0m8b/iP/22CWu0vCdIgHyDsmhcec0cY3J4GPkfYhTDcN58glfBE+Nh3Vway2LHvIKG3TNr7BRwHfJMg3/50WM7jY/vPAH5E8MW1m6C75YJa5671nIHfJ2gEjZ7zvwGvzPqcM7wX8wm+mM5MKOcRBG08T4XH3AG8PLZ/Zfj+PUPwy+R6YEls//cIPrvxy2kJj7MM9ZbJfIk+GCIiUiBqUBURKSAFdxGRAlJwFxEpIAV3EZEC6orZ1Y488khftmxZp4shIpIrd91118/cfXHSvq4I7suWLWNoaKjTxRARyRUzeyRtn9IyIiIFpOAuIlJACu4iIgWk4C4iUkAK7iIiBdQVvWWaYePWYTbcupPHR0ZZ2ldm3ZrlrF3VX/uOIiIFVIjgvnHrMJfduJ3RsXEAhkdGuezG7QAK8CIyKxUiLbPh1p2TgT0yOjbOhlt3dqhEIiKdVYjg/vhI8lKjadtFRIquEMF9aV+5ru0iIkVXiOC+bs1yyqXeKdvKpV7WrVneoRKJiHRWIRpUo0ZT9ZYREQkUIrhDEOAVzEVEAoVIy4iIyFQK7iIiBVQzuJvZMWZ2u5n9xMx2mNl7w+2Hm9l3zeyn4d9F4XYzs781swfM7F4zO6XVT0JERKbKUnM/CLzP3U8ETgUuNrMTgUHgNnc/HrgtvA3wGuD48HIR8Nmml1pERKqqGdzd/Ql3vzu8/hxwH9APnANcHR52NbA2vH4O8CUP3An0mdlRTS+5iIikqivnbmbLgFXAFmCJuz8R7toNLAmv9wO7Ynd7LNxWea6LzGzIzIb27NlTZ7FFRKSazMHdzF4AfB24xN1/Ed/n7g54PQ/s7le5+4C7DyxenLi+q4iINChTcDezEkFgv8bdbww3PxmlW8K/T4Xbh4FjYnc/OtwmIiJtkqW3jAFfAO5z90/Fdt0EXBhevxD4Rmz728NeM6cCz8bSNyIi0gZZRqiuBv4I2G5m28JtHwDWA9eb2buAR4ALwn23AK8FHgD2Au9saolFRKSmmsHd3e8ALGX3mQnHO3DxDMslIiIzoBGqIiIFpOAuIlJACu4iIgWk4C4iUkAK7iIiBaTgLiJSQAruIiIFpOAuIlJACu4iIgWk4C4iUkBZ5paZtTZuHWbDrTt5fGSUpX1l1q1ZztpV06amFxHpOgruKTZuHeayG7czOjYOwPDIKJfduB1AAV5Eup7SMik23LpzMrBHRsfG2XDrzg6VSEQkOwX3FI+PjNa1XUSkmyi4p1jaV65ru4hIN1FwT7FuzXLKpd4p28qlXtatWd6hEomIZKcG1RRRo6l6y4hIHuU2uLejm+LaVf0K5iKSS7kM7uqmKCJSXS5z7uqmKCJSXS5r7nnqpqhRriLSCbmsueelm2KUPhoeGcU5lD7auHW400UTkYLLZXDPSzdFpY9EpFNymZbJSzfFPKWPRKRYchncIR/dFJf2lRlOCOTdlj4SkeLJZVomL/KSPhKR4sltzT0P8pI+EpHiUXBvsTykj0SkeJSWEREpIAV3EZECUnAXESkgBXcRkQJSg6q0hObUEeksBXdpOk3JLNJ5SstI02lOHZHOK2TNXSmBztKcOiKdV7iau6bZ7by8TMksUmSFC+5KCXSe5tQR6byawd3MvmhmT5nZj2PbLjezYTPbFl5eG9t3mZk9YGY7zWxNqwqeRimBzlu7qp+Pn3sS/X1lDOjvK/Pxc09SakykjbLk3P8J+DTwpYrtf+3ufxXfYGYnAm8GVgBLgX81s5e6+zhtoml2u4Pm1BHprJo1d3f/PvB0xvOdA3zV3fe7+0PAA8DLZ1C+uiklICIys5z7u83s3jBtsyjc1g/sih3zWLhtGjO7yMyGzGxoz549MyjGVEoJiIg03hXys8CfAx7+/STw3+o5gbtfBVwFMDAw4A2WI5FSAiIy2zVUc3f3J9193N0ngH/gUOplGDgmdujR4TYREWmjhoK7mR0Vu/kHQNST5ibgzWY2z8yOA44H/t/MiigiIvWqmZYxs2uB04Ajzewx4MPAaWa2kiAt8zDwxwDuvsPMrgd+AhwELm5nTxkREQmYe1PT3Q0ZGBjwoaGhThdDRCRXzOwudx9I2lfIuWW6iea5EZFOmHXBvZ3BVlPfikinFG5umWraPamY5rkRkU6ZVcG93cFW89yISKfMquDe7mCrqW9FpFNmVXBvd7DVPDci0imzKri3O9hqnhsR6ZRZ1VsmCqrt7JqoeW5EpBNmVXAHBVsRmR0KE9w1WEhE5JBCBHcNFhIRmaoQwb1a/3UF9/rpV5BI/hUiuGuwUPPoV5BIMRSiK6QGCzWPpkwQKYZCBHcNFmoe/QoSKYZCBHcNFmoe/QoSKYZC5NxB/debZd2a5VNy7qBfQSJ5VJjgLs3RiVG8ItJ8Cu4yjX4FieRfIXLuIiIylYK7iEgBKbiLiBSQgruISAEpuIuIFJCCu4hIASm4i4gUkIK7iEgBKbiLiBSQRqjGaJEKESkKBfeQFqkQkSJRcA/NZKk+1fhFpNsouIcaXaRCNX4R6UZqUA01ukiFlqUTkW6k4B6qd6m+jVuHWb1+E8Nalk5EupDSMqF6FqmoTMUk0bJ0ItJJCu4xWRepSErFxGlZOhHpNAX3BlRLufSrt4yIdAEF9wYs7Ssn5tr7+8psHjyjAyUSEZmqZoOqmX3RzJ4ysx/Hth1uZt81s5+GfxeF283M/tbMHjCze83slFYWPk3U2Hnc4M2sXr+JjVuHm3r+ehtfRUTaLUtvmX8CzqrYNgjc5u7HA7eFtwFeAxwfXi4CPtucYmYXNXYOj4ziHOp33swAv3ZVPx8/9yT6+8oYQY394+eepFSMiHSNmmkZd/++mS2r2HwOcFp4/Wrge8D7w+1fcncH7jSzPjM7yt2faFaBa5nJSNN6ZG18FRHphEb7uS+JBezdwJLwej+wK3bcY+G2aczsIjMbMrOhPXv2NFiM6RodaSoiUiQzHsQU1tK9gftd5e4D7j6wePHimRZjUqMjTUVEiqTR4P6kmR0FEP59Ktw+DBwTO+7ocFvbdHNjZ6sbekVEIo0G95uAC8PrFwLfiG1/e9hr5lTg2Xbm26F7Gzvb0dArIhKp2aBqZtcSNJ4eaWaPAR8G1gPXm9m7gEeAC8LDbwFeCzwA7AXe2YIy19SNjZ3taugVEYFsvWXekrLrzIRjHbh4poUqIjX0ikg7aVbINlFDr4i006ybfqBy1aTTT1jM7ffvyTQT5ExWW1q3Zvm0mSS7paFXRIqnMME9S/BNWjXpy3c+Ork/bRWlZqy2VM+UwiIiM2VBmryzBgYGfGhoqOH7J82vXi71TuslU21xjbjKCcDS7qeJwkSkk8zsLncfSNpXiJx71qXusjZeVh6nxlARyZtCBPeswTdr42XlcWoMFZG8KURwzxp8k0avVir12rRGznVrllPqsanH9Uw/TkSkWxQiuGedcqBy9OqiBaXpL0BaE4TVuC0i0kUKEdzrmXJg7ap+Ng+ewUPrz2bB3DlMVOwfm/BpufoNt+5kbHxq1B8bn36ciEi3KExXyEamHMiaq1eDqojkTSFq7o3KmqtXg6qI5M2sDu5Zc/XdPI2wiEiSWR3cs+bq167q57yX9dNrQStqrxnnvaz7Zp4UEYkUJufeqGq5+mhKg+GRUYxDHWnG3fn6XcMMvPhwBXgR6UqzIrjH551ZWC5hBiN7x2pOFBaf0qCyh6TmYheRblb44F4ZpEdGxyb3VZsALGlKg0pRb5mZzhgpItJshQ/utYJ0Wg08SzfHHjOWDd48JWXTyIyRIiLNVvgG1SxBOumYLN0cx8MZNdNSNiIinVLImns8TdJjNhmE00SBvDI3X+q1aSNTs9IAp/ZSakxkqsIF98oce63AHk0UlpSbL/UYixaUeGbv2JTUSxYa4NQ+zVhMRaRocpuW2bh1mNXrN3Hc4M2sXr+JjVuHgWwNoVOEETvpfmMTzoK5c+jvK9cV2DXAqb2yzucvMpvksuZeraZWbzokmigs7X5ZVm4CJmv2/V2UEpgtqQrN/SMyXS6De7Wa2tK+cuaAHImCX7336zVjwr0rA+dsSlWkvXdKjclslsu0TLWaWpYFOSo52WvokXKpl09ecDIPrT+bzYNndF3AnEmqIi3l1a0094/IdLmsuVerqUVBNt7r5dnRsbpy5rXUk3rpVGqk0VRFHmv8le95N/6SEmm3XAb3dWuWTwlAMLWmFp8v5oMbt/PlOx+t6/y9Zrxo4fzEL5D+vjKbB8/IdJ5OBspGUxXVavzdHCwbmc9fpMhymZapZ+Wla7fsSjxHNMNjknH3pvzU72QvjkbLr8ZJkWLIZc0dstfU0vq5j7vTmzLAqdesKT/1OxkoGy2/GidFiiG3wT0rM0gbxzS/1MPzB6b3iX/L7xwDzPynfqcDZSPlr5XyEpF8yGVaph7lOelP8fkD4/T2GFGCpteMt516LB9de1JTHjuPvTjqSXmJSPcqfM19dGyi6v7xCZ9sJI16thw3eHNTelzktReHGidF8s+8xtwr7TAwMOBDQ0NNO1+9E4elKZd6VWsVka5lZne5+0DivqIF98ruhzOVZRTqbBnmLyLdpVpwL1xapu6Jw2qIav2VfdQ/uHE7127ZNe1XQR4G/RSBvlBFqitccG9lN8Ooj/rQI09XHRiVddBPPED1LSjhDs+OVl/bVfI5ilak3XIb3NNqbvVMAJbWz72a4ZHR1IFRcfUO839mb7a1XSW/o2hF2imXXSE3bh1m3Q33MDwyOjnp17ob7gm21zFx2Lw56aNUq8nyhdDIMP84zUeeTqNoRWrLZc39im/umLb83di4c8U3d7D1Q68GDnU/nNMDab0h945N0GPBrJDNbFeeyTD/eo9ptW7MbXd6cJhIHsyo5m5mD5vZdjPbZmZD4bbDzey7ZvbT8O+i5hT1kHgKo9b2Ca9eO59wWLqwXHWumXpkHfSTJRB1OlhFqaP4L6TLbtze8SmA8zg4TKTdmlFzP93dfxa7PQjc5u7rzWwwvP3+JjxOJvWuoQpBDXkmFfdeM97yO8fw0bUnTdZ0L71uG0v7ypx+wmJuv3/PtJpv0jD/uG4IVt2a287r4DCRdmpFWuYc4LTw+tXA92hjcG+kK+RMBjo9vP7syetJvTjivWqSGkqz9JapJzXSzDRKN+e2u2kUbTemrkRmGtwd+I6ZOfA5d78KWOLuT4T7dwNLku5oZhcBFwEce+yxdT3oogWlxBTMogWlhgJPo4G9UpYvlnjNN0uAqqfbX7O7CNbKbSuoqVumdK+Z9pZ5hbufArwGuNjMXhnf6cHw18TI6e5XufuAuw8sXry4rgf98OtXUOqdmiMv9Roffv2K1Dx1s3LqcdEZo2XpsnbBjI7LspxdPXPCN3v++Gq57aR8/Lqv3cOqj3wnN8vzNUMn5+wXqWZGwd3dh8O/TwH/ArwceNLMjgII/z4100JWWruqnw1vPHnKzIUb3njyZC47KSB98oKTm10MnKmNjln1miUGx0uv28ayisCYdt6k7c1Oo1SbITIpqI1NOM/sHeuqxtdW6+bUlcxuDadlzOwwoMfdnwuvvxr4CHATcCGwPvz7jWYUtFJaSqNaY9sl121rahl6zRo657h7YnCMfuLEf9pXW1CkUiu6CKa9zlmCVzc0vraaumVKt5pJzn0J8C8WBJk5wFfc/dtm9iPgejN7F/AIcMHMi1mfyoAUpT+yMtL/aeMazdX395VrBscoMFZbSQqm5r0XlkuUem3KGICZ9rqZ6UjgotdgtbiJdKuGg7u7PwhMy3W4+8+BM2dSqGbauHWYdV+7h7GJ7IHYCYJSb48xHrufhfuiv42I/vE33LqzZnB8fGSU/pQg2t9XntaYNzI6RqnHWLSgxMjemc9RU62xsFZXzkjRa7DqlindKpcjVJOk1TAvv2lHXYE94gQLeRw2t5e9B8annHPZ4M11nSst2NYKjtHxaTXDtLz3grlzJkfqzkS1xsLNg2dMHhP9anj+wMGm/mrIi27qlikSKURwr1bDHBlNHs0KcOWbVtYMsPvGJngo1pe9Ee7w0Pqzpw1wOuXYhdz54DOJqZcoMFarGV6aku9vViqkVmNhUvpLNViR7pDb4F5rtaUs3dHigTMtRdKMPvAjo2OJX0CVjxmle/orAmNazbDVjXn1nl81WJHukdtZIePdCNMCcK2c9ur1mxh65OmqxyR1j+8rl7IWddIl122rmZ+OAvvmwTMyBclWz7GiOVxE8iuXwb1Zqy1F0wNU+xIoz5n+Er3u5KNm/NjVypRVtX7ozdDq84tI6+QyLdPO7nV7xyY4bvDmKTnk2+/f07LHq3ckbbVUSDNy4Eq1iORTLoN7Wi64kZWVsohGXF5y3bamD4SqlNR/vZHArDlPRGa3XKZl1q1ZTqmnYm6ZHuOTFwRTEuTdqo98h3Vfm7rSVL1D+TXnicjslsuaO3Bo1q6K26efsLjq4tV5kDTj5ejYOJdct40Nt+7MVIsv+pwn6nYpM+IOB/fDwX3BZWw05fYojIXbpt1u4H7j+6eWw3rg3UNwxEua/hRzGdw33LozcZm9bq2VRumiypGtpV7jsLlzqvbFr1SZXql3eoCkbox5C5RKOdUv03s8PtbkoJbxuBktlZNzPgE/f0DBPZJW+6ynp0k7Va7SVPkPdtzgzXV9vOPplXqmB0jqxpjHQNm2FaImxsOAlBas0gJZrcA4fd/+fXsZP7CXuX6AOZay6O8MrA0vzAf2EUzn15Ip/XKkdx7MmQ9z5kFpPswph9fL4fb502/Hj6u8neW43rnQ055seC6De9ZJq7rFtVt2cc2dj06utlSpL2XxkWoeHxmta3qAtNpaXYHSvb0/YaPr4wemFGMzBEGq0j7g8rpexq4xL7rS/GUHsuuZkyFw1QhqqUEubd986OmtXTapm3kLepfUa2BgwIeGhjIfv3HrMOtuuGdaaqb9nBLjzOcA8znAPBtjHgeYT/B3no0d2sfYtNsv6B3jzF//Fe5+cDc2vj/cHuybb7H7Vdwu24HaRZMmsObU4mrc79zP380jz06wnxL7mMvBsM4VDWibqbRfhgYznlpDOsvM7nL3gaR9uay5A4yPO/PZz2dLV/J7Pdsnf8o+5kdy6/hvM4eDzGGCXsYp2Tjn9f6gwyVO8SC8BCAvlZfeeU0LapnvN2felKHCSTN9lnqMDeef3LWppGq2PvvTxODbrMZvzTk/O+UyuF9+0w4mgGPtKU7vvWfKvqPtZ5zf+z3G6eVgeBlvYY/PA97LfuZO1rr2e4n9zGUfJfZ5sL3y9j7mTjl2fvkwnhyF/T513+R1ShzwEqPM5fCFC3nPmhWsPeXYaflyCPLqSaNI044972X9fP2u4Uzn6CopvaXyqNXBdyZzzuetsV0OyWVwj3qX/Icfw0v3Xc04PYx3YdU3y6CqaJKwq26oPed8f1+ZTbGf6fXMJZ6WW7/9/j18/NyTcvUPXK23VDeXO02rF/xodM75PDa2yyG5DO5xB6h/Eq+ZqidoV5tSeMo/cIbmg6Sf6VmnH0g7/eMjo7mbYqBoffjbseBHI+9x23olSUvkPri3U5SuSJtHPe70ExZP+6eNess8Ozp14Y7V6zdlWlCknp/pWRud85h3LWIOuRu/YIv2JTrbKLhnZAaGc+l12xLnj68UTS6W5Z82S7fOen+mX/HNHTUDe6nHWLdmee7yqlq3tD2K+CU6m+QyuJuR2F+8ldyDGSIh2wIeSbWbtCBaLc0TLdYdD7hZgnGWfvMTwNAjT09pUM06AraTtG5pe+hLNN9yGdy7oGt+TQsrFvSoTJMMj4yy7oagp0+1L4vKfsjNbOQan3Cu2fLotNczywjYTgfSbkxjFI2+RPMtl8G9x6CBNa/bKuqWHdV8k37ejo07V3xzB/0pP3+TZrjM2sjVVy5lmrMm7Xul1ghY/YPPDvoSza9cTvnb7YEdgrTIyiu+w7ob7qmaU39m71hdy9lVm1dn9fpNk9MCX/6GFdOmRa7H0r6yGtREciyXwT0vRkbHMk2RUM9ydtUas+Lzvq9d1c+G80+ePGdamF9Q6kn9Ykl7LDWoiXS/XKZlsqYc8iBabDvrz99afefjaZP4OdOG7P/Fub8FTM2rnn7C4slUUuU0xWpQE8mHXAb3y9+wYlqgyqNSj3H5G1YA03vAnH7CYm6/f8+0hqx4I1dauidtsFN0v6TGsfiXQPzLw2EywPf3lVl2RJn3XX8Pl1y3jV6zyemMRaS75HJWSIC3/sMP2fyfT7eoRK3XHwuuSbXqSknzvaxev0vIR2wAAAgYSURBVCm1IbbR2QSrnTNtlau3nXpsxwN8N3bZFKmmGZ/ZarNC5jLn/sGN23Mf2DcPnjH5Rl5+046av0KiZfbijab1NMRmVa0R9dotuxL3pW1vl+jXRiNrzm7cOszq9Zs4bvDmKa+tSCvN5DObVS7TMtfkfI3UygA6k2X2ID3VkjXVE1dtVGJaGijLoK64ZteyG+2yqYmxpFPa0c04lzX3zieSZmamvU3ig4zSJNUMvnznozVrCtV+DfRacp+bpO1pNeJW1Fga7bJZ7R9MpJXa0c04lzX3vDv9hMUzPsfwyCjLBm+e0pslXvNMClyVkmoK1X4NDD3ydGLO/S2/c8yU29VqxK2osTQ6B4r68UuntGPenlzW3PMumlQsMoOxRtN+xUSBMmuAqhz8BEGA3zx4Bg+tP3tK28BH157E2049drKm3muW2JhaLYC3IqA22vagfvzSKa1oL6uUy5r72049NrEGmRfDI6O85LJbJrsRNrtHZ1TjzrqIeD255o+uPalmz5hqAbwVNZZG50DRxFjSKe2YtyeXwT0KLtdu2VV3Y163GHef/IJKm1umUQvLpZqDnSqlpUYaafysFsBbFVAbmQNFE2NJJ7V63p7cpmUGXnw4L1o4v+uWzly0oERfuYQRDO2v5dotuzLl4MulXq5808rEycQqjY1PJE5p8LZTj616/8oad6ONn9V+ctYz1UI7pKWgRPIulzX3pMWeu8Uze8eIYvr+g7V/VYy7c/O9TyTu6wnnra+sUdZ67s8fCPal1QzSBipVpkYabfzMMhpWQVSktXIZ3LP0BOmkcE2PTCmjXrPUhTUmHB6umM89y/QDkbSUStbUSNr5s6SQFMBFOiuXaZkidVWr7EaYRZRK6CsnLw7eVy5VTanUSo1EfdTTpPV3F5Hu0bKau5mdBfwN0At83t3XN+vc9fQE6Va9Zpz6a4umdYuMK9fI2SdNoBZNRlYrpZJWs86S8sprI7bIbNKSmruZ9QJ/B7wGOBF4i5md2KzzJzXYdbNoPvX+vjJXvmklD68/m09ecDJ3P/ps1S+p+TWeY+Wc7f19ZTacfzJrV/U3ddRmpSyNuiLSWa2qub8ceMDdHwQws68C5wA/acbJKxvseqosMN2oeXN62H9woinn6ltQYuuHXj1lW5YgOpJhkeu0GnizR21G1A9cJB9alXPvB+JTBT4WbptkZheZ2ZCZDe3Zk56aSBPvwjbRgjTBJ877raojR0s9xqIFyTnvSklBOku7wUwG9jR71CZ0vtuiiGTXsQZVd7/K3QfcfWDx4pnNtZIlCPb3lTOnE/r7yqxd1c+nLlg5mfKI91+P0h9bP/RqrnzTypopoqTy1SrzTGvIjfYnT/tSuPJNK9UPXCRHWpWWGQbi3UCODre1RK3RmPFAWauxMH5slu58lV0Tsy5Ll1Tm+IpHzRgpqVGbIrNXS1ZiMrM5wH8AZxIE9R8Bf+juO5KOb2QlpkrxPt0LyyXMgnRIM+Y4b7Qctc6n1YNEZCaqrcTUsmX2zOy1wJUEXSG/6O4fSzu2GcFdRGS2qRbcW9bP3d1vAW5p1flFRCRdLkeoiohIdQruIiIFpOAuIlJACu4iIgXUst4ydRXCbA/wSB13ORL4WYuKk1d6TabTazKdXpNkeX1dXuzuiaNAuyK418vMhtK6/8xWek2m02synV6TZEV8XZSWEREpIAV3EZECymtwv6rTBehCek2m02synV6TZIV7XXKZcxcRkeryWnMXEZEqFNxFRAooV8HdzM4ys51m9oCZDXa6PJ1iZseY2e1m9hMz22Fm7w23H25m3zWzn4Z/F3W6rO1kZr1mttXMvhXePs7MtoSfl+vMbG6ny9huZtZnZjeY2f1mdp+Z/a4+J3Zp+H/zYzO71szmF/Gzkpvg3upFt3PmIPA+dz8ROBW4OHwtBoHb3P144Lbw9mzyXuC+2O1PAH/t7r8OPAO8qyOl6qy/Ab7t7icAJxO8PrP2c2Jm/cB7gAF3/02CKcnfTAE/K7kJ7sQW3Xb3A0C06Pas4+5PuPvd4fXnCP5h+wlej6vDw64G1namhO1nZkcDZwOfD28bcAZwQ3jIrHo9AMxsIfBK4AsA7n7A3UeYxZ+T0BygHC4qtAB4ggJ+VvIU3Gsuuj0bmdkyYBWwBVji7k+Eu3YDSzpUrE64EvgzYCK8fQQw4u4Hw9uz8fNyHLAH+McwXfV5MzuMWfw5cfdh4K+ARwmC+rPAXRTws5Kn4C4VzOwFwNeBS9z9F/F9HvRxnRX9XM3sdcBT7n5Xp8vSZeYApwCfdfdVwPNUpGBm0+cEIGxfOIfgi28pcBhwVkcL1SJ5Cu5tXXS725lZiSCwX+PuN4abnzSzo8L9RwFPdap8bbYaeIOZPUyQrjuDINfcF/70htn5eXkMeMzdt4S3byAI9rP1cwLwKuAhd9/j7mPAjQSfn8J9VvIU3H8EHB+2as8laAS5qcNl6ogwn/wF4D53/1Rs103AheH1C4FvtLtsneDul7n70e6+jOBzscnd3wrcDrwxPGzWvB4Rd98N7DKz5eGmM4GfMEs/J6FHgVPNbEH4fxS9JoX7rORqhGo9i24XmZm9AvgBsJ1DOeYPEOTdrweOJZhC+QJ3f7ojhewQMzsN+J/u/joz+zWCmvzhwFbgbe6+v5PlazczW0nQyDwXeBB4J0GlbtZ+TszsCuBNBL3OtgL/nSDHXqjPSq6Cu4iIZJOntIyIiGSk4C4iUkAK7iIiBaTgLiJSQAruIiIFpOAuIlJACu4iIgX0/wE3bwDHVYJL1AAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"Bocz7OAAK_c6"},"source":["# CBIO"]},{"cell_type":"markdown","metadata":{"id":"gEOZnd-HAcu0"},"source":["## Load"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8tKaEh7HAhXc","executionInfo":{"status":"ok","timestamp":1620501304470,"user_tz":240,"elapsed":603,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"789bc962-7de9-419b-9b78-f52c121bcd57"},"source":["merged_path = Path(eileen_path + \"merged_mut-count.csv\")\n","merged_df = pd.read_csv(merged_path)\n","\n","patients_df = merged_df[['PATIENT_ID', 'OS_MONTHS', 'BINARY_SURVIVAL']]\n","smol_cbio_df = merged_df[['PATIENT_ID',\t'SEX',\t'AGE',\t'PRIMARY_SITE_PATIENT',\t'RACE',\t'SMOKING_PACK_YEARS',\t'CLIN_N_STAGE',\t'CLIN_T_STAGE',\t'CLIN_M_STAGE',\t'CLINICAL_STAGE',\t'OS_MONTHS',\t'BINARY_SURVIVAL', 'HPV_STATUS']]\n","\n","# for col in smol_cbio_df.columns:\n","#   if type(smol_cbio_df[col][0]) != str: continue \n","#   if any(~smol_cbio_df[col].str.contains(\"[Not Available]\")):\n","#     print(col)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["AGE\n","RACE\n","SMOKING_PACK_YEARS\n","CLIN_T_STAGE\n","CLIN_M_STAGE\n","OS_MONTHS\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"734o2wcyHnyi"},"source":["one_hot_feats = [ 'PRIMARY_SITE_PATIENT', 'SEX', 'RACE', 'CLINICAL_STAGE']\n","one_hots = smol_cbio_df[one_hot_feats]\n","one_hots = pd.get_dummies(one_hots)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PtYWHFBFS1aw"},"source":["num_df = smol_cbio_df[[\"AGE\", \"SMOKING_PACK_YEARS\"]].replace('[Not Available]', np.nan)\n","num_df = pd.DataFrame(SimpleImputer(missing_values=np.nan, strategy='median').fit(num_df).transform(num_df), columns=['AGE', 'SMOKING_PACK_YEARS'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lb37leveLOwc"},"source":["X = pd.concat([one_hots, num_df, smol_cbio_df[['HPV_STATUS']]], axis = 1) #pd.concat([one_hots, conts], axis=1)\n","y = smol_cbio_df['BINARY_SURVIVAL']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mkuFtjLdAeQm"},"source":["## Model "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0seTlqs1AbHS","executionInfo":{"status":"ok","timestamp":1620501969603,"user_tz":240,"elapsed":2125,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"3418f13d-ce99-4db6-b345-f00aa3debadc"},"source":["clf = RandomForestClassifier()\n","cv_results = cross_validate(clf, X, y, cv=5, scoring='roc_auc')\n","print(\"RFC\", np.mean(cv_results['test_score']))\n","\n","#try l1 regularization\n","clf = LogisticRegression(penalty=\"l1\", solver='liblinear', C=1, class_weight='balanced')\n","cv_results = cross_validate(clf, X, y, cv=5, scoring='roc_auc')\n","print(\"LR\", np.mean(cv_results['test_score']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["RFC 0.5225429292929293\n","LR 0.5516683501683501\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XXF9BzzNVZO_"},"source":["RFC: 0.51-0.54 \\\\\n","LR: 0.54\n","\n","Without Race: \\\\\n","RFC - 0.53 \\\\\n","LR - 0.551\n","\n","Without Sex: \\\\\n","RFC - 0.53\n","LR - 0.551\n","\n","Without Race & Sex: \\\\\n","RFC - 0.53 \\\\\n","LR - 0.551"]},{"cell_type":"markdown","metadata":{"id":"yP6AlzvpXCr6"},"source":["# Microenv"]},{"cell_type":"code","metadata":{"id":"_CCSLLi2XB6i","executionInfo":{"status":"ok","timestamp":1621022499618,"user_tz":240,"elapsed":539,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}}},"source":["judith_path = Path(eileen_path + \"relevant_judith.csv\")\n","judith_df = pd.read_csv(judith_path)\n","judith_df.drop(columns='Unnamed: 0', inplace=True)\n","judith_df = judith_df.replace('[Not Available]', np.nan)\n","\n","merged_path = Path(eileen_path + \"merged_mut-count.csv\")\n","merged_df = pd.read_csv(merged_path)\n","\n","judith_df = judith_df[judith_df.PATIENT_ID.isin(merged_df.PATIENT_ID)]\n","patients_df = merged_df[merged_df.PATIENT_ID.isin(judith_df.PATIENT_ID)][['PATIENT_ID', 'BINARY_SURVIVAL']]"],"execution_count":77,"outputs":[]},{"cell_type":"code","metadata":{"id":"3GGQHw12Y2rF","executionInfo":{"status":"ok","timestamp":1621022501468,"user_tz":240,"elapsed":500,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}}},"source":["patients_df = patients_df.sort_values(\"PATIENT_ID\")\n","judith_df = judith_df.sort_values(\"PATIENT_ID\")\n","judith_df = judith_df.drop(columns=\"PATIENT_ID\")\n","\n","\n","features = pd.DataFrame(SimpleImputer(missing_values=np.nan, strategy='median').fit(judith_df).transform(judith_df)) \n"],"execution_count":78,"outputs":[]},{"cell_type":"code","metadata":{"id":"fVEjdQvBaiJW","executionInfo":{"status":"ok","timestamp":1621022938045,"user_tz":240,"elapsed":334,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}}},"source":["X, y = features, patients_df['BINARY_SURVIVAL']\n"],"execution_count":107,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"emc3Y664XBxF","executionInfo":{"status":"ok","timestamp":1621022942607,"user_tz":240,"elapsed":3417,"user":{"displayName":"Joseph Morales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitsVxy_P6e8H9CtoVx1d1-mlwWqv5zHT-Wwu_cwQ=s64","userId":"05367319364657857003"}},"outputId":"f3d936a9-ccc4-47eb-94c3-7d155eff18a0"},"source":["clf = RandomForestClassifier()\n","cv_results = cross_validate(clf, X, y, cv=5, scoring='roc_auc')\n","print(\"RFC\", np.mean(cv_results['test_score']))\n","\n","#try l1 regularization \n","clf = LogisticRegression(penalty='l1', solver='liblinear', C=1, class_weight='balanced')\n","cv_results = cross_validate(clf, X, y, cv=5, scoring='roc_auc')\n","print(\"LR\", np.mean(cv_results['test_score']))"],"execution_count":108,"outputs":[{"output_type":"stream","text":["RFC 0.5934449155037391\n","LR 0.6082321574968634\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"svrpN5z7fzGR"},"source":["Microenv: \\\\\n","RFC - 0.59 \\\\\n","LR - 0.61"]},{"cell_type":"code","metadata":{"id":"YUWtGayiawmI"},"source":[""],"execution_count":null,"outputs":[]}]}